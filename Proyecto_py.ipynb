{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTkaFx5k5YYZKPlQOddwEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ViviRomoReyes/Deber/blob/main/Proyecto_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhIo41dXz1L"
      },
      "source": [
        "# **PROYECTO INTERCICLO VIVIANA ROMO REYES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v85PKvyX2Tv"
      },
      "source": [
        "# **MODELO DE APRENDIZAJE AUTOMÁTICO DE EXTREMO A EXTREMO CON PYTHON**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRRD0ofAX8JA"
      },
      "source": [
        "Utilizamos la biblioteca Streamlit en python con el sigt comando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKiSW5o3X8_5",
        "outputId": "02dced99-feee-4aaa-b375-9247674d0046"
      },
      "source": [
        "pip install streamlit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.84.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.3)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.6.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.18)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.0.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (4.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: debugpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata<4; python_version < \"3.8.0\" in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (3.10.1)\n",
            "Collecting ipython>=7.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/b2/733ea4551a04866bbcfbbade4d9d2c82c829cf1cc6fac1ac5974b8c7f756/ipython-7.25.0-py3-none-any.whl (786kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit) (4.0.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4; python_version < \"3.8.0\"->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (3.4.1)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (57.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/34/1d9880ac1339ad4c6697b330e7a507584105613751318249d9e820faa25f/prompt_toolkit-3.0.19-py3-none-any.whl (368kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.7.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.19 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 6.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.25.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit, ipython\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed ipython-7.25.0 prompt-toolkit-3.0.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPx4ir2YVgL"
      },
      "source": [
        "Antes de realizar el modelo de extremo a extremo vamos a implementar un modelo de predicción de precios de \"DOGECOIN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJkXhe2gYWkn"
      },
      "source": [
        "**PREDICCIÓN DEL PRECIO DE DOGECOIN CON APRENDIZAJE AUTOMÁTICO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yDWhu1UYcs3"
      },
      "source": [
        "Para poder obtener el dataset ingresamos a Yahoo Finanzas luego buscamos Dogecoin e ingreamos a datos históricos por último damos click en descargar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkpfR48oYflZ",
        "outputId": "fd72de8c-f38f-4f7c-ce8e-a1d2dcc7f127"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from seaborn import regression\n",
        "sns.set()\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "data = pd.read_csv(\"Dogecoin.csv\")\n",
        "print(data.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         Date      Open      High       Low     Close  Adj Close        Volume\n",
            "0  2020-06-03  0.192283  0.195824  0.191579  0.194541   0.194541  1.431394e+10\n",
            "1  2020-06-04  0.194546  0.201372  0.194089  0.198500   0.198500  1.464478e+10\n",
            "2  2020-06-05  0.198575  0.198809  0.195110  0.196144   0.196144  1.427010e+10\n",
            "3  2020-06-06  0.196118  0.196929  0.193914  0.195498   0.195498  1.329847e+10\n",
            "4  2020-06-07  0.195499  0.197185  0.193320  0.196806   0.196806  1.420881e+10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TApeG0CmbTAY"
      },
      "source": [
        "En la columna cerrar contiene los valores que a futuro queremos predecir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XReGtvCCbdDO"
      },
      "source": [
        "A continuación veremos los valores históricos de los precios de cierre de \"DOGECOIN\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "i-K9ZYzObnNA",
        "outputId": "225843a7-0a40-4a79-a51e-9d3bcff471f8"
      },
      "source": [
        "data.dropna()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.title(\"DogeCoin Price INR\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close\")\n",
        "plt.plot(data[\"Close\"])\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYE9U4A1b26n"
      },
      "source": [
        "Para entrenar un modelo de aprendizaje automático para predecir los precios futuros de \"DOGECOIN\" utilizaremos la biblioteca autots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_jBR7HYcKf2",
        "outputId": "c08a1b1d-b2f3-4266-b49b-4480abe35f85"
      },
      "source": [
        "pip install autots"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autots in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.* in /usr/local/lib/python3.7/dist-packages (from autots) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.25.* in /usr/local/lib/python3.7/dist-packages (from autots) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from autots) (1.19.5)\n",
            "Requirement already satisfied: statsmodels>=0.10.* in /usr/local/lib/python3.7/dist-packages (from autots) (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.*->autots) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.*->autots) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.*->autots) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.*->autots) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.10.*->autots) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.*->autots) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho5_wRGXecuQ"
      },
      "source": [
        "Ahora podremos ver los precios a futuro de DOGECOIN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0nTrY3IrVdF",
        "outputId": "66f43fe3-9165-4668-c441-8a16ca38dea7"
      },
      "source": [
        "from autots import AutoTS\n",
        "model = AutoTS(forecast_length=10, frequency='infer', ensemble='simple', drop_data_older_than_periods=200)\n",
        "model = model.fit(data, date_col='Date', value_col='Close', id_col=None)\n",
        "\n",
        "prediction = model.predict()\n",
        "forecast = prediction.forecast\n",
        "print(\"DogeCoin Price Prediction\")\n",
        "print(forecast)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inferred frequency is: D\n",
            "Old data dropped by `drop_data_older_than_periods`.\n",
            "Model Number: 1 with model AverageValueNaive in generation 0 of 20\n",
            "Model Number: 2 with model AverageValueNaive in generation 0 of 20\n",
            "Model Number: 3 with model AverageValueNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 3: AverageValueNaive\n",
            "Model Number: 4 with model DatepartRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 5 with model DatepartRegression in generation 0 of 20\n",
            "Model Number: 6 with model DatepartRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 7 with model DatepartRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 19:48:44.451315: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 19:48:45.957629: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-14 19:48:45.970170: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-07-14 19:48:45.970214: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7a87e6281fad): /proc/driver/nvidia/version does not exist\n",
            "2021-07-14 19:48:46.574058: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-14 19:48:46.575382: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6/6 [==============================] - 4s 4ms/step - loss: 0.3462\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.3313\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.3179\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2983\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2866\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2758\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2499\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2422\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2324\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2252\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2093\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.2045\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.1928\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.1779\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1724\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1654\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1665\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1593\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1633\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.1607\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1458\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1477\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1490\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1521\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1524\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1534\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1412\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1387\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1542\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1278\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1403\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1309\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1329\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1396\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1221\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1299\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1238\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1322\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1250\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1203\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1275\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1174\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1213\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1204\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1276\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1184\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1188\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1153\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1122\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.1191\n",
            "Model Number: 8 with model ETS in generation 0 of 20\n",
            "Model Number: 9 with model ETS in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 9: ETS\n",
            "Model Number: 10 with model GLM in generation 0 of 20\n",
            "Model Number: 11 with model GLM in generation 0 of 20\n",
            "Model Number: 12 with model GLS in generation 0 of 20\n",
            "Model Number: 13 with model GLS in generation 0 of 20\n",
            "Model Number: 14 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 14: GluonTS\n",
            "Model Number: 15 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 15: GluonTS\n",
            "Model Number: 16 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 16: GluonTS\n",
            "Model Number: 17 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 17: GluonTS\n",
            "Model Number: 18 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 18: GluonTS\n",
            "Model Number: 19 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 19: GluonTS\n",
            "Model Number: 20 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 21 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 22 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 23 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 24 with model SeasonalNaive in generation 0 of 20\n",
            "Model Number: 25 with model SeasonalNaive in generation 0 of 20\n",
            "Model Number: 26 with model SeasonalNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 26: SeasonalNaive\n",
            "Model Number: 27 with model SeasonalNaive in generation 0 of 20\n",
            "Model Number: 28 with model UnobservedComponents in generation 0 of 20\n",
            "Model Number: 29 with model UnobservedComponents in generation 0 of 20\n",
            "Model Number: 30 with model UnobservedComponents in generation 0 of 20\n",
            "Model Number: 31 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 31: VAR\n",
            "Model Number: 32 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 32: VAR\n",
            "Model Number: 33 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 33: VAR\n",
            "Model Number: 34 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 34: VECM\n",
            "Model Number: 35 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 35: VECM\n",
            "Model Number: 36 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 36: VECM\n",
            "Model Number: 37 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 37: VECM\n",
            "Model Number: 38 with model WindowRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 39 with model ZeroesNaive in generation 0 of 20\n",
            "Model Number: 40 with model UnivariateRegression in generation 0 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 40: UnivariateRegression\n",
            "Model Number: 41 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 41: VAR\n",
            "Model Number: 42 with model FBProphet in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 42: FBProphet\n",
            "Model Number: 43 with model GLM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 43: GLM\n",
            "Model Number: 44 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 45 with model AverageValueNaive in generation 0 of 20\n",
            "Model Number: 46 with model ZeroesNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 46: ZeroesNaive\n",
            "Model Number: 47 with model RollingRegression in generation 0 of 20\n",
            "Model Number: 48 with model FBProphet in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 48: FBProphet\n",
            "Model Number: 49 with model GLM in generation 0 of 20\n",
            "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 49: GLM\n",
            "Model Number: 50 with model WindowRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:521: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in multiply\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/generalized_linear_model.py:1163: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in multiply\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 50: WindowRegression\n",
            "Model Number: 51 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 51: GluonTS\n",
            "Model Number: 52 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 52: VECM\n",
            "Model Number: 53 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 53: GluonTS\n",
            "Model Number: 54 with model DatepartRegression in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 54: DatepartRegression\n",
            "Model Number: 55 with model DatepartRegression in generation 0 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 56 with model ETS in generation 0 of 20\n",
            "Model Number: 57 with model GLS in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 57: GLS\n",
            "Model Number: 58 with model DatepartRegression in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 58: DatepartRegression\n",
            "Model Number: 59 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 59: VAR\n",
            "Model Number: 60 with model GLS in generation 0 of 20\n",
            "Model Number: 61 with model ZeroesNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 61: ZeroesNaive\n",
            "Model Number: 62 with model VAR in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 62: VAR\n",
            "Model Number: 63 with model VECM in generation 0 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 63: VECM\n",
            "Model Number: 64 with model ZeroesNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 64: ZeroesNaive\n",
            "Model Number: 65 with model UnobservedComponents in generation 0 of 20\n",
            "Model Number: 66 with model GluonTS in generation 0 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 66: GluonTS\n",
            "Model Number: 67 with model GLS in generation 0 of 20\n",
            "Model Number: 68 with model AverageValueNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 68: AverageValueNaive\n",
            "Model Number: 69 with model FBProphet in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 69: FBProphet\n",
            "Model Number: 70 with model GLS in generation 0 of 20\n",
            "Model Number: 71 with model DatepartRegression in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 71: DatepartRegression\n",
            "Model Number: 72 with model UnobservedComponents in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 72: UnobservedComponents\n",
            "Model Number: 73 with model UnobservedComponents in generation 0 of 20\n",
            "Model Number: 74 with model ZeroesNaive in generation 0 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 74: ZeroesNaive\n",
            "Model Number: 75 with model LastValueNaive in generation 0 of 20\n",
            "Model Number: 76 with model UnivariateRegression in generation 0 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 76: UnivariateRegression\n",
            "Model Number: 77 with model SeasonalNaive in generation 0 of 20\n",
            "Model Number: 78 with model GLS in generation 0 of 20\n",
            "Model Number: 79 with model GLS in generation 0 of 20\n",
            "New Generation: 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 80 with model LastValueNaive in generation 1 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 80: LastValueNaive\n",
            "Model Number: 81 with model LastValueNaive in generation 1 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 81: LastValueNaive\n",
            "Model Number: 82 with model LastValueNaive in generation 1 of 20\n",
            "Model Number: 83 with model UnobservedComponents in generation 1 of 20\n",
            "Model Number: 84 with model UnobservedComponents in generation 1 of 20\n",
            "Model Number: 85 with model UnobservedComponents in generation 1 of 20\n",
            "Model Number: 86 with model GLS in generation 1 of 20\n",
            "Model Number: 87 with model GLS in generation 1 of 20\n",
            "Model Number: 88 with model GLS in generation 1 of 20\n",
            "Model Number: 89 with model ETS in generation 1 of 20\n",
            "Model Number: 90 with model ETS in generation 1 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 90: ETS\n",
            "Model Number: 91 with model ETS in generation 1 of 20\n",
            "Model Number: 92 with model ETS in generation 1 of 20\n",
            "Model Number: 93 with model AverageValueNaive in generation 1 of 20\n",
            "Model Number: 94 with model AverageValueNaive in generation 1 of 20\n",
            "Model Number: 95 with model AverageValueNaive in generation 1 of 20\n",
            "Model Number: 96 with model SeasonalNaive in generation 1 of 20\n",
            "Model Number: 97 with model SeasonalNaive in generation 1 of 20\n",
            "Model Number: 98 with model SeasonalNaive in generation 1 of 20\n",
            "Model Number: 99 with model SeasonalNaive in generation 1 of 20\n",
            "Model Number: 100 with model DatepartRegression in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 101 with model DatepartRegression in generation 1 of 20\n",
            "Model Number: 102 with model DatepartRegression in generation 1 of 20\n",
            "Model Number: 103 with model RollingRegression in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 104 with model RollingRegression in generation 1 of 20\n",
            "Model Number: 105 with model RollingRegression in generation 1 of 20\n",
            "Model Number: 106 with model RollingRegression in generation 1 of 20\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 5s 6ms/step - loss: 0.8644\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8635\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8638\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8635\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8634\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8638\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8634\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8633\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8637\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8634\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8633\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8631\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8634\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8639\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8631\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8628\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8635\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8628\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8623\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8625\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8623\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8621\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8618\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8610\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8604\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8606\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8599\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8592\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8613\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8585\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8565\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8566\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8579\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8538\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8549\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8560\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8548\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8554\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8544\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8529\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8533\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8499\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8533\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8552\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.8511\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.8542\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8519\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.8537\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8524\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8502\n",
            "Model Number: 107 with model ZeroesNaive in generation 1 of 20\n",
            "Model Number: 108 with model ZeroesNaive in generation 1 of 20\n",
            "Model Number: 109 with model ZeroesNaive in generation 1 of 20\n",
            "Model Number: 110 with model WindowRegression in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 110: WindowRegression\n",
            "Model Number: 111 with model WindowRegression in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 112 with model WindowRegression in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 113 with model WindowRegression in generation 1 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 113: WindowRegression\n",
            "Model Number: 114 with model GLM in generation 1 of 20\n",
            "Model Number: 115 with model GLM in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 116 with model GLM in generation 1 of 20\n",
            "Model Number: 117 with model GLM in generation 1 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 117: GLM\n",
            "Model Number: 118 with model GluonTS in generation 1 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 118: GluonTS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 119 with model GluonTS in generation 1 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 119: GluonTS\n",
            "Model Number: 120 with model GluonTS in generation 1 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 120: GluonTS\n",
            "Model Number: 121 with model GluonTS in generation 1 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 121: GluonTS\n",
            "Model Number: 122 with model VAR in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 122: VAR\n",
            "Model Number: 123 with model VAR in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 123: VAR\n",
            "Model Number: 124 with model VAR in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 124: VAR\n",
            "Model Number: 125 with model VAR in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 125: VAR\n",
            "Model Number: 126 with model VECM in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 126: VECM\n",
            "Model Number: 127 with model VECM in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 127: VECM\n",
            "Model Number: 128 with model VECM in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 128: VECM\n",
            "Model Number: 129 with model VECM in generation 1 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 129: VECM\n",
            "Model Number: 130 with model UnivariateRegression in generation 1 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 130: UnivariateRegression\n",
            "Model Number: 131 with model UnivariateRegression in generation 1 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 131: UnivariateRegression\n",
            "Model Number: 132 with model UnivariateRegression in generation 1 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 132: UnivariateRegression\n",
            "Model Number: 133 with model UnivariateRegression in generation 1 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 133: UnivariateRegression\n",
            "Model Number: 134 with model FBProphet in generation 1 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 134: FBProphet\n",
            "Model Number: 135 with model FBProphet in generation 1 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -32.1745\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       46.2267   0.000218811        80.394           1           1      140   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     133       46.2338   3.03184e-06       53.7266   4.532e-08       0.001      222  LS failed, Hessian reset \n",
            "     157        46.234    7.6842e-09       64.9315      0.7696      0.7696      259   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 136 with model FBProphet in generation 1 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 136: FBProphet\n",
            "Model Number: 137 with model FBProphet in generation 1 of 20\n",
            "Initial log joint probability = -3.57623\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       323.333   7.46871e-06       99.4563      0.2136      0.2136      123   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     138       323.336   3.58124e-09       93.5781      0.2037      0.2037      178   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "New Generation: 2 of 20\n",
            "Model Number: 138 with model LastValueNaive in generation 2 of 20\n",
            "Model Number: 139 with model LastValueNaive in generation 2 of 20\n",
            "Model Number: 140 with model LastValueNaive in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 141 with model DatepartRegression in generation 2 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 142 with model DatepartRegression in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 143 with model DatepartRegression in generation 2 of 20\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]................................"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "......................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]........."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".............................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]............................................................................................................................................"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "..........\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]................................................................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".....\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear].................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "....................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear].........................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "............................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]..............................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".......................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]........................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".............................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]...................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "..................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear].................................................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "....................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear].............................................................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "........\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = 0.000000\n",
            "nSV = 0\n",
            "Model Number: 144 with model UnobservedComponents in generation 2 of 20\n",
            "Model Number: 145 with model UnobservedComponents in generation 2 of 20\n",
            "Model Number: 146 with model UnobservedComponents in generation 2 of 20\n",
            "Model Number: 147 with model GLS in generation 2 of 20\n",
            "Model Number: 148 with model GLS in generation 2 of 20\n",
            "Model Number: 149 with model GLS in generation 2 of 20\n",
            "Model Number: 150 with model ETS in generation 2 of 20\n",
            "Model Number: 151 with model ETS in generation 2 of 20\n",
            "Model Number: 152 with model ETS in generation 2 of 20\n",
            "Model Number: 153 with model ETS in generation 2 of 20\n",
            "Model Number: 154 with model AverageValueNaive in generation 2 of 20\n",
            "Model Number: 155 with model AverageValueNaive in generation 2 of 20\n",
            "Model Number: 156 with model AverageValueNaive in generation 2 of 20\n",
            "Model Number: 157 with model SeasonalNaive in generation 2 of 20\n",
            "Model Number: 158 with model SeasonalNaive in generation 2 of 20\n",
            "Model Number: 159 with model SeasonalNaive in generation 2 of 20\n",
            "Model Number: 160 with model SeasonalNaive in generation 2 of 20\n",
            "Model Number: 161 with model FBProphet in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -24.7399\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99        103.97   0.000122212       72.5276      0.8539      0.8539      124   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       104.139   1.14413e-06       71.0463      0.7147     0.07147      262   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     299       104.178   2.07653e-07       72.6579      0.7425      0.7425      395   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     310       104.178   3.43779e-07       71.0315   4.047e-09       0.001      450  LS failed, Hessian reset \n",
            "     318       104.178   4.52434e-09       74.8687      0.1498      0.1498      460   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 162 with model FBProphet in generation 2 of 20\n",
            "Initial log joint probability = -7.55882\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       221.855     0.0312584       79.1707           1           1      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     105       221.967     0.0012134       69.9492   2.606e-05       0.001      196  LS failed, Hessian reset \n",
            "     199       222.217   2.55519e-06       87.8621           1           1      328   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     203       222.217   3.57491e-06       101.884   4.979e-08       0.001      370  LS failed, Hessian reset \n",
            "     219       222.217   4.95365e-09       69.9006      0.3031      0.3031      396   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 163 with model FBProphet in generation 2 of 20\n",
            "Initial log joint probability = -8.37093\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      61       250.101   0.000570512       63.1148   7.052e-06       0.001      115  LS failed, Hessian reset \n",
            "      99       250.149   1.14164e-06        55.836      0.4683      0.4683      170   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     142       251.141    0.00270843       96.7886   3.313e-05       0.001      286  LS failed, Hessian reset \n",
            "     199       251.684   1.95195e-06       80.6665      0.2423           1      356   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     221       251.684   4.52082e-09       75.2202      0.2683      0.2683      384   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 164 with model FBProphet in generation 2 of 20\n",
            "Initial log joint probability = -28.87\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      75       120.682   0.000588832        91.989    7.24e-06       0.001      145  LS failed, Hessian reset \n",
            "      99       120.751   4.92123e-06       88.9373      0.3544      0.3544      175   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       120.889   7.60271e-09       97.1556      0.4662      0.4662      294   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 165 with model RollingRegression in generation 2 of 20\n",
            "Model Number: 166 with model RollingRegression in generation 2 of 20\n",
            "Model Number: 167 with model RollingRegression in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 168 with model RollingRegression in generation 2 of 20\n",
            "Model Number: 169 with model ZeroesNaive in generation 2 of 20\n",
            "Model Number: 170 with model ZeroesNaive in generation 2 of 20\n",
            "Model Number: 171 with model ZeroesNaive in generation 2 of 20\n",
            "Model Number: 172 with model WindowRegression in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 173 with model WindowRegression in generation 2 of 20\n",
            "Template Eval Error: ValueError('at least one array or dtype is required') in model 173: WindowRegression\n",
            "Model Number: 174 with model WindowRegression in generation 2 of 20\n",
            "Template Eval Error: ValueError('at least one array or dtype is required') in model 174: WindowRegression\n",
            "Model Number: 175 with model WindowRegression in generation 2 of 20\n",
            "Model Number: 176 with model GLM in generation 2 of 20\n",
            "Model Number: 177 with model GLM in generation 2 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 178 with model GLM in generation 2 of 20\n",
            "Model Number: 179 with model GLM in generation 2 of 20\n",
            "Model Number: 180 with model GluonTS in generation 2 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 180: GluonTS\n",
            "Model Number: 181 with model GluonTS in generation 2 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 181: GluonTS"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Number: 182 with model GluonTS in generation 2 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 182: GluonTS\n",
            "Model Number: 183 with model GluonTS in generation 2 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 183: GluonTS\n",
            "Model Number: 184 with model VAR in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 184: VAR\n",
            "Model Number: 185 with model VAR in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 185: VAR\n",
            "Model Number: 186 with model VAR in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 186: VAR\n",
            "Model Number: 187 with model VAR in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 187: VAR\n",
            "Model Number: 188 with model VECM in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 188: VECM\n",
            "Model Number: 189 with model VECM in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 189: VECM\n",
            "Model Number: 190 with model VECM in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 190: VECM\n",
            "Model Number: 191 with model VECM in generation 2 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 191: VECM\n",
            "Model Number: 192 with model UnivariateRegression in generation 2 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 192: UnivariateRegression\n",
            "Model Number: 193 with model UnivariateRegression in generation 2 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 193: UnivariateRegression\n",
            "Model Number: 194 with model UnivariateRegression in generation 2 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 194: UnivariateRegression\n",
            "Model Number: 195 with model UnivariateRegression in generation 2 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 195: UnivariateRegression\n",
            "New Generation: 3 of 20\n",
            "Model Number: 196 with model LastValueNaive in generation 3 of 20\n",
            "Model Number: 197 with model DatepartRegression in generation 3 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 198 with model DatepartRegression in generation 3 of 20\n",
            "Model Number: 199 with model DatepartRegression in generation 3 of 20\n",
            "Model Number: 200 with model UnobservedComponents in generation 3 of 20\n",
            "Model Number: 201 with model UnobservedComponents in generation 3 of 20\n",
            "Model Number: 202 with model UnobservedComponents in generation 3 of 20\n",
            "Model Number: 203 with model SeasonalNaive in generation 3 of 20\n",
            "Model Number: 204 with model SeasonalNaive in generation 3 of 20\n",
            "Model Number: 205 with model SeasonalNaive in generation 3 of 20\n",
            "Model Number: 206 with model SeasonalNaive in generation 3 of 20\n",
            "Model Number: 207 with model GLS in generation 3 of 20\n",
            "Model Number: 208 with model GLS in generation 3 of 20\n",
            "Model Number: 209 with model GLS in generation 3 of 20\n",
            "Model Number: 210 with model GLM in generation 3 of 20\n",
            "Model Number: 211 with model GLM in generation 3 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 212 with model GLM in generation 3 of 20\n",
            "Model Number: 213 with model GLM in generation 3 of 20\n",
            "Model Number: 214 with model ETS in generation 3 of 20\n",
            "Model Number: 215 with model ETS in generation 3 of 20\n",
            "Model Number: 216 with model ETS in generation 3 of 20\n",
            "Model Number: 217 with model ETS in generation 3 of 20\n",
            "Model Number: 218 with model AverageValueNaive in generation 3 of 20\n",
            "Model Number: 219 with model AverageValueNaive in generation 3 of 20\n",
            "Model Number: 220 with model AverageValueNaive in generation 3 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 221 with model FBProphet in generation 3 of 20\n",
            "Initial log joint probability = -2.64578\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      83       404.198   1.46707e-05       71.9833   1.829e-07       0.001      145  LS failed, Hessian reset \n",
            "      99         404.2   5.73577e-07       65.2096      0.2574           1      165   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     117         404.2   1.87265e-08       82.8302        0.28           1      192   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 222 with model FBProphet in generation 3 of 20\n",
            "Initial log joint probability = -40.7944\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       121.895    1.2835e-06       93.1994      0.8369      0.8369      124   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     118       121.895   5.07582e-09       86.5164      0.2596      0.2596      150   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 223 with model FBProphet in generation 3 of 20\n",
            "Initial log joint probability = -7.27654\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       217.759     0.0114219       73.8054           1           1      133   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     121       218.555   0.000415244       88.7285    4.94e-06       0.001      198  LS failed, Hessian reset \n",
            "     199       218.804   1.13165e-05       77.6877      0.3688      0.3688      303   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     257       218.807   4.79898e-06       76.9309   5.503e-08       0.001      408  LS failed, Hessian reset \n",
            "     283       218.807   1.48458e-08       70.3377      0.0772           1      442   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 224 with model FBProphet in generation 3 of 20\n",
            "Initial log joint probability = -29.9176\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      87       52.8395   0.000447869       72.1808   6.036e-06       0.001      141  LS failed, Hessian reset \n",
            "      99       52.8587   6.78911e-05       62.0959           1           1      157   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     131       52.8647   4.74174e-09       60.1944     0.04278           1      209   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 225 with model RollingRegression in generation 3 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 225: RollingRegression\n",
            "Model Number: 226 with model RollingRegression in generation 3 of 20\n",
            "Model Number: 227 with model RollingRegression in generation 3 of 20\n",
            "Model Number: 228 with model RollingRegression in generation 3 of 20\n",
            "Model Number: 229 with model ZeroesNaive in generation 3 of 20\n",
            "Model Number: 230 with model ZeroesNaive in generation 3 of 20\n",
            "Model Number: 231 with model ZeroesNaive in generation 3 of 20\n",
            "Model Number: 232 with model WindowRegression in generation 3 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 232: WindowRegression\n",
            "Model Number: 233 with model WindowRegression in generation 3 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 234 with model WindowRegression in generation 3 of 20\n",
            "Model Number: 235 with model WindowRegression in generation 3 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 236 with model GluonTS in generation 3 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 236: GluonTS\n",
            "Model Number: 237 with model GluonTS in generation 3 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 237: GluonTS\n",
            "Model Number: 238 with model GluonTS in generation 3 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 238: GluonTS\n",
            "Model Number: 239 with model GluonTS in generation 3 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 239: GluonTS\n",
            "Model Number: 240 with model VAR in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 240: VAR\n",
            "Model Number: 241 with model VAR in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 241: VAR\n",
            "Model Number: 242 with model VAR in generation 3 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 242: VAR\n",
            "Model Number: 243 with model VAR in generation 3 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 243: VAR\n",
            "Model Number: 244 with model VECM in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 244: VECM\n",
            "Model Number: 245 with model VECM in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 245: VECM\n",
            "Model Number: 246 with model VECM in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 246: VECM\n",
            "Model Number: 247 with model VECM in generation 3 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 247: VECM\n",
            "Model Number: 248 with model UnivariateRegression in generation 3 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 248: UnivariateRegression\n",
            "Model Number: 249 with model UnivariateRegression in generation 3 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 249: UnivariateRegression\n",
            "Model Number: 250 with model UnivariateRegression in generation 3 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 250: UnivariateRegression\n",
            "Model Number: 251 with model UnivariateRegression in generation 3 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 251: UnivariateRegression\n",
            "New Generation: 4 of 20\n",
            "Model Number: 252 with model FBProphet in generation 4 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -8.04323\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      87        208.26    0.00365605       97.7423   3.839e-05       0.001      151  LS failed, Hessian reset \n",
            "      99       208.484   5.80279e-05       76.5849      0.2187      0.2187      169   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     155       208.504   5.52018e-06       63.1576   6.696e-08       0.001      276  LS failed, Hessian reset \n",
            "     195       208.504   7.81882e-07       59.2164    9.74e-09       0.001      360  LS failed, Hessian reset \n",
            "     199       208.504   2.12495e-07        65.543       2.848      0.2848      366   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     209       208.504    3.0923e-09         84.78      0.1417      0.1417      380   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 253 with model FBProphet in generation 4 of 20\n",
            "Initial log joint probability = -44.1683\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       39.1131   0.000980454       103.092      0.7854      0.7854      128   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     102       39.2016    0.00064775       73.8948   8.758e-06       0.001      168  LS failed, Hessian reset \n",
            "     149       39.2563   0.000107042       65.8928   9.756e-07       0.001      259  LS failed, Hessian reset \n",
            "     199         39.26   4.07326e-06        83.821       2.851           1      326   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     216       39.2607   3.12351e-06       63.8435   3.631e-08       0.001      381  LS failed, Hessian reset \n",
            "     241       39.2609   8.24992e-09       81.0723      0.3342      0.3342      414   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 254 with model DatepartRegression in generation 4 of 20\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 255 with model DatepartRegression in generation 4 of 20\n",
            "Model Number: 256 with model DatepartRegression in generation 4 of 20\n",
            "Model Number: 257 with model LastValueNaive in generation 4 of 20\n",
            "Model Number: 258 with model LastValueNaive in generation 4 of 20\n",
            "Model Number: 259 with model SeasonalNaive in generation 4 of 20\n",
            "Model Number: 260 with model SeasonalNaive in generation 4 of 20\n",
            "Model Number: 261 with model SeasonalNaive in generation 4 of 20\n",
            "Model Number: 262 with model SeasonalNaive in generation 4 of 20\n",
            "Model Number: 263 with model GLS in generation 4 of 20\n",
            "Model Number: 264 with model GLS in generation 4 of 20\n",
            "Model Number: 265 with model GLS in generation 4 of 20\n",
            "Model Number: 266 with model UnobservedComponents in generation 4 of 20\n",
            "Model Number: 267 with model UnobservedComponents in generation 4 of 20\n",
            "Model Number: 268 with model UnobservedComponents in generation 4 of 20\n",
            "Model Number: 269 with model GLM in generation 4 of 20\n",
            "Model Number: 270 with model GLM in generation 4 of 20\n",
            "Model Number: 271 with model GLM in generation 4 of 20\n",
            "Model Number: 272 with model GLM in generation 4 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 272: GLM\n",
            "Model Number: 273 with model ETS in generation 4 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters.py:712: ConvergenceWarning:\n",
            "\n",
            "Optimization failed to converge. Check mle_retvals.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 274 with model ETS in generation 4 of 20\n",
            "Model Number: 275 with model ETS in generation 4 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters.py:712: ConvergenceWarning:\n",
            "\n",
            "Optimization failed to converge. Check mle_retvals.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 276 with model ETS in generation 4 of 20\n",
            "Model Number: 277 with model AverageValueNaive in generation 4 of 20\n",
            "Model Number: 278 with model AverageValueNaive in generation 4 of 20\n",
            "Model Number: 279 with model WindowRegression in generation 4 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 279: WindowRegression\n",
            "Model Number: 280 with model WindowRegression in generation 4 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 280: WindowRegression\n",
            "Model Number: 281 with model RollingRegression in generation 4 of 20\n",
            "Model Number: 282 with model RollingRegression in generation 4 of 20\n",
            "Model Number: 283 with model RollingRegression in generation 4 of 20\n",
            "Model Number: 284 with model RollingRegression in generation 4 of 20\n",
            "Model Number: 285 with model ZeroesNaive in generation 4 of 20\n",
            "Model Number: 286 with model ZeroesNaive in generation 4 of 20\n",
            "Model Number: 287 with model ZeroesNaive in generation 4 of 20\n",
            "Model Number: 288 with model GluonTS in generation 4 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 288: GluonTS\n",
            "Model Number: 289 with model GluonTS in generation 4 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 289: GluonTS\n",
            "Model Number: 290 with model GluonTS in generation 4 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 290: GluonTS\n",
            "Model Number: 291 with model GluonTS in generation 4 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 291: GluonTS\n",
            "Model Number: 292 with model VAR in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 292: VAR\n",
            "Model Number: 293 with model VAR in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 293: VAR\n",
            "Model Number: 294 with model VAR in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 294: VAR\n",
            "Model Number: 295 with model VAR in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 295: VAR\n",
            "Model Number: 296 with model VECM in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 296: VECM\n",
            "Model Number: 297 with model VECM in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 297: VECM\n",
            "Model Number: 298 with model VECM in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 298: VECM\n",
            "Model Number: 299 with model VECM in generation 4 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 299: VECM\n",
            "Model Number: 300 with model UnivariateRegression in generation 4 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 300: UnivariateRegression\n",
            "Model Number: 301 with model UnivariateRegression in generation 4 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 301: UnivariateRegression\n",
            "Model Number: 302 with model UnivariateRegression in generation 4 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 302: UnivariateRegression\n",
            "Model Number: 303 with model UnivariateRegression in generation 4 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 303: UnivariateRegression\n",
            "New Generation: 5 of 20\n",
            "Model Number: 304 with model FBProphet in generation 5 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -7.00016\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99        273.11   0.000290868       82.2335      0.3178           1      136   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     155       273.189    8.1333e-09       79.2348      0.1921           1      216   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 305 with model FBProphet in generation 5 of 20\n",
            "Initial log joint probability = -10.3106\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      91       168.314   6.50654e-09        103.71      0.3284      0.3284      116   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: UFuncTypeError(<ufunc 'multiply'>, 'same_kind', dtype('float64'), dtype('int64'), 2) in model 305: FBProphet\n",
            "Model Number: 306 with model FBProphet in generation 5 of 20\n",
            "Initial log joint probability = -6.76631\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       232.589   2.16691e-06       103.489      0.8243      0.8243      135   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       232.739   2.29059e-06       90.3238           1           1      255   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     214       232.741   7.90497e-06       96.9456   7.085e-08       0.001      314  LS failed, Hessian reset \n",
            "     240       232.742   4.73084e-09        92.285      0.2829      0.2829      347   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 307 with model DatepartRegression in generation 5 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 308 with model DatepartRegression in generation 5 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 309 with model DatepartRegression in generation 5 of 20\n",
            "Model Number: 310 with model RollingRegression in generation 5 of 20\n",
            "Model Number: 311 with model RollingRegression in generation 5 of 20\n",
            "Model Number: 312 with model RollingRegression in generation 5 of 20\n",
            "Model Number: 313 with model RollingRegression in generation 5 of 20\n",
            "Model Number: 314 with model LastValueNaive in generation 5 of 20\n",
            "Model Number: 315 with model LastValueNaive in generation 5 of 20\n",
            "Model Number: 316 with model LastValueNaive in generation 5 of 20\n",
            "Model Number: 317 with model SeasonalNaive in generation 5 of 20\n",
            "Model Number: 318 with model SeasonalNaive in generation 5 of 20\n",
            "Model Number: 319 with model SeasonalNaive in generation 5 of 20\n",
            "Model Number: 320 with model SeasonalNaive in generation 5 of 20\n",
            "Model Number: 321 with model UnobservedComponents in generation 5 of 20\n",
            "Model Number: 322 with model UnobservedComponents in generation 5 of 20\n",
            "Model Number: 323 with model UnobservedComponents in generation 5 of 20\n",
            "Model Number: 324 with model GLS in generation 5 of 20\n",
            "Model Number: 325 with model GLS in generation 5 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 325: GLS\n",
            "Model Number: 326 with model GLM in generation 5 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 326: GLM\n",
            "Model Number: 327 with model GLM in generation 5 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 328 with model GLM in generation 5 of 20\n",
            "Model Number: 329 with model GLM in generation 5 of 20\n",
            "Model Number: 330 with model ETS in generation 5 of 20\n",
            "Model Number: 331 with model ETS in generation 5 of 20\n",
            "Model Number: 332 with model ETS in generation 5 of 20\n",
            "Model Number: 333 with model ETS in generation 5 of 20\n",
            "Model Number: 334 with model AverageValueNaive in generation 5 of 20\n",
            "Model Number: 335 with model AverageValueNaive in generation 5 of 20\n",
            "Model Number: 336 with model AverageValueNaive in generation 5 of 20\n",
            "Model Number: 337 with model WindowRegression in generation 5 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 337: WindowRegression\n",
            "Model Number: 338 with model WindowRegression in generation 5 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 338: WindowRegression\n",
            "Model Number: 339 with model WindowRegression in generation 5 of 20\n",
            "Model Number: 340 with model WindowRegression in generation 5 of 20\n",
            "Model Number: 341 with model ZeroesNaive in generation 5 of 20\n",
            "Model Number: 342 with model ZeroesNaive in generation 5 of 20\n",
            "Model Number: 343 with model ZeroesNaive in generation 5 of 20\n",
            "Model Number: 344 with model GluonTS in generation 5 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 344: GluonTS\n",
            "Model Number: 345 with model GluonTS in generation 5 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 345: GluonTS\n",
            "Model Number: 346 with model GluonTS in generation 5 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 346: GluonTS\n",
            "Model Number: 347 with model GluonTS in generation 5 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 347: GluonTS\n",
            "Model Number: 348 with model VAR in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 348: VAR\n",
            "Model Number: 349 with model VAR in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 349: VAR\n",
            "Model Number: 350 with model VAR in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 350: VAR\n",
            "Model Number: 351 with model VAR in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 351: VAR\n",
            "Model Number: 352 with model VECM in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 352: VECM\n",
            "Model Number: 353 with model VECM in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 353: VECM\n",
            "Model Number: 354 with model VECM in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 354: VECM\n",
            "Model Number: 355 with model VECM in generation 5 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 355: VECM\n",
            "Model Number: 356 with model UnivariateRegression in generation 5 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 356: UnivariateRegression\n",
            "Model Number: 357 with model UnivariateRegression in generation 5 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 357: UnivariateRegression\n",
            "Model Number: 358 with model UnivariateRegression in generation 5 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 358: UnivariateRegression\n",
            "Model Number: 359 with model UnivariateRegression in generation 5 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 359: UnivariateRegression\n",
            "New Generation: 6 of 20\n",
            "Model Number: 360 with model FBProphet in generation 6 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -7.28713\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       214.757   0.000554849       72.8941      0.5966      0.5966      130   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     131       215.062    0.00281158       79.2561   2.748e-05       0.001      207  LS failed, Hessian reset \n",
            "     199       215.176   9.87276e-06       85.2163           1           1      295   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     239       215.178   9.50145e-09       72.6183           1           1      351   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 361 with model DatepartRegression in generation 6 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 362 with model DatepartRegression in generation 6 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 363 with model DatepartRegression in generation 6 of 20\n",
            "Model Number: 364 with model RollingRegression in generation 6 of 20\n",
            "Model Number: 365 with model RollingRegression in generation 6 of 20\n",
            "Model Number: 366 with model RollingRegression in generation 6 of 20\n",
            "Model Number: 367 with model RollingRegression in generation 6 of 20\n",
            "Model Number: 368 with model LastValueNaive in generation 6 of 20\n",
            "Model Number: 369 with model LastValueNaive in generation 6 of 20\n",
            "Model Number: 370 with model SeasonalNaive in generation 6 of 20\n",
            "Model Number: 371 with model SeasonalNaive in generation 6 of 20\n",
            "Model Number: 372 with model SeasonalNaive in generation 6 of 20\n",
            "Model Number: 373 with model SeasonalNaive in generation 6 of 20\n",
            "Model Number: 374 with model UnobservedComponents in generation 6 of 20\n",
            "Model Number: 375 with model UnobservedComponents in generation 6 of 20\n",
            "Model Number: 376 with model UnobservedComponents in generation 6 of 20\n",
            "Model Number: 377 with model GLM in generation 6 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 378 with model GLM in generation 6 of 20\n",
            "Model Number: 379 with model GLM in generation 6 of 20\n",
            "Model Number: 380 with model GLM in generation 6 of 20\n",
            "Model Number: 381 with model ETS in generation 6 of 20\n",
            "Model Number: 382 with model ETS in generation 6 of 20\n",
            "Model Number: 383 with model ETS in generation 6 of 20\n",
            "Model Number: 384 with model ETS in generation 6 of 20\n",
            "Model Number: 385 with model AverageValueNaive in generation 6 of 20\n",
            "Model Number: 386 with model AverageValueNaive in generation 6 of 20\n",
            "Model Number: 387 with model AverageValueNaive in generation 6 of 20\n",
            "Model Number: 388 with model WindowRegression in generation 6 of 20\n",
            "Model Number: 389 with model WindowRegression in generation 6 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1019: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 389: WindowRegression\n",
            "Model Number: 390 with model WindowRegression in generation 6 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 390: WindowRegression\n",
            "Model Number: 391 with model WindowRegression in generation 6 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\") in model 391: WindowRegression\n",
            "Model Number: 392 with model ZeroesNaive in generation 6 of 20\n",
            "Model Number: 393 with model ZeroesNaive in generation 6 of 20\n",
            "Model Number: 394 with model ZeroesNaive in generation 6 of 20\n",
            "Model Number: 395 with model GluonTS in generation 6 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 395: GluonTS\n",
            "Model Number: 396 with model GluonTS in generation 6 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 396: GluonTS\n",
            "Model Number: 397 with model GluonTS in generation 6 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 397: GluonTS\n",
            "Model Number: 398 with model GluonTS in generation 6 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 398: GluonTS\n",
            "Model Number: 399 with model VAR in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 399: VAR\n",
            "Model Number: 400 with model VAR in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 400: VAR\n",
            "Model Number: 401 with model VAR in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 401: VAR\n",
            "Model Number: 402 with model VECM in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 402: VECM\n",
            "Model Number: 403 with model VECM in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 403: VECM\n",
            "Model Number: 404 with model VECM in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 404: VECM\n",
            "Model Number: 405 with model VECM in generation 6 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 405: VECM\n",
            "Model Number: 406 with model UnivariateRegression in generation 6 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 406: UnivariateRegression\n",
            "Model Number: 407 with model UnivariateRegression in generation 6 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 407: UnivariateRegression\n",
            "Model Number: 408 with model UnivariateRegression in generation 6 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 408: UnivariateRegression\n",
            "Model Number: 409 with model UnivariateRegression in generation 6 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 409: UnivariateRegression\n",
            "New Generation: 7 of 20\n",
            "Model Number: 410 with model FBProphet in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -27.982\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      90       67.5895   5.63927e-06        96.282   5.314e-08       0.001      144  LS failed, Hessian reset \n",
            "      99       67.5898   2.51048e-07       90.9126      0.3521           1      154   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     109       67.5898   5.87028e-09       101.914         0.2         0.2      166   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 411 with model FBProphet in generation 7 of 20\n",
            "Initial log joint probability = -25.3027\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       77.3816   1.03286e-05       95.9203           1           1      116   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     100       77.3817   1.23091e-06       98.8834   1.283e-08       0.001      174  LS failed, Hessian reset \n",
            "     121       77.3819   8.56683e-09       100.157      0.3036      0.3036      200   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 412 with model FBProphet in generation 7 of 20\n",
            "Initial log joint probability = -7.58814\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      83       236.586    0.00037023       102.035   3.757e-06       0.001      145  LS failed, Hessian reset \n",
            "      99       236.611    4.3608e-06        111.07      0.4456      0.4456      167   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     109       236.617   4.29156e-05       97.5045   4.115e-07       0.001      210  LS failed, Hessian reset \n",
            "     154       236.619   4.05339e-07        100.56   4.247e-09       0.001      301  LS failed, Hessian reset \n",
            "     167       236.619   8.08043e-09       95.1178      0.3955      0.3955      319   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 413 with model DatepartRegression in generation 7 of 20\n",
            "Model Number: 414 with model DatepartRegression in generation 7 of 20\n",
            "Model Number: 415 with model DatepartRegression in generation 7 of 20\n",
            "Model Number: 416 with model RollingRegression in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 417 with model RollingRegression in generation 7 of 20\n",
            "Model Number: 418 with model RollingRegression in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 419 with model RollingRegression in generation 7 of 20\n",
            "Model Number: 420 with model LastValueNaive in generation 7 of 20\n",
            "Model Number: 421 with model LastValueNaive in generation 7 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 421: LastValueNaive\n",
            "Model Number: 422 with model LastValueNaive in generation 7 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 422: LastValueNaive\n",
            "Model Number: 423 with model SeasonalNaive in generation 7 of 20\n",
            "Model Number: 424 with model SeasonalNaive in generation 7 of 20\n",
            "Model Number: 425 with model SeasonalNaive in generation 7 of 20\n",
            "Model Number: 426 with model SeasonalNaive in generation 7 of 20\n",
            "Model Number: 427 with model UnobservedComponents in generation 7 of 20\n",
            "Model Number: 428 with model UnobservedComponents in generation 7 of 20\n",
            "Model Number: 429 with model UnobservedComponents in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/minpack.py:808: OptimizeWarning:\n",
            "\n",
            "Covariance of the parameters could not be estimated\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 430 with model GLS in generation 7 of 20\n",
            "Model Number: 431 with model GLM in generation 7 of 20\n",
            "Model Number: 432 with model GLM in generation 7 of 20\n",
            "Model Number: 433 with model GLM in generation 7 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 433: GLM\n",
            "Model Number: 434 with model GLM in generation 7 of 20\n",
            "Model Number: 435 with model ETS in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 436 with model ETS in generation 7 of 20\n",
            "Model Number: 437 with model ETS in generation 7 of 20\n",
            "Model Number: 438 with model ETS in generation 7 of 20\n",
            "Model Number: 439 with model AverageValueNaive in generation 7 of 20\n",
            "Model Number: 440 with model AverageValueNaive in generation 7 of 20\n",
            "Model Number: 441 with model AverageValueNaive in generation 7 of 20\n",
            "Model Number: 442 with model WindowRegression in generation 7 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 442: WindowRegression\n",
            "Model Number: 443 with model WindowRegression in generation 7 of 20\n",
            "Model Number: 444 with model WindowRegression in generation 7 of 20\n",
            "Model Number: 445 with model WindowRegression in generation 7 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 445: WindowRegression\n",
            "Model Number: 446 with model ZeroesNaive in generation 7 of 20\n",
            "Model Number: 447 with model ZeroesNaive in generation 7 of 20\n",
            "Model Number: 448 with model GluonTS in generation 7 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 448: GluonTS\n",
            "Model Number: 449 with model GluonTS in generation 7 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 449: GluonTS\n",
            "Model Number: 450 with model GluonTS in generation 7 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 450: GluonTS\n",
            "Model Number: 451 with model GluonTS in generation 7 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 451: GluonTS\n",
            "Model Number: 452 with model VAR in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 452: VAR\n",
            "Model Number: 453 with model VAR in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 453: VAR\n",
            "Model Number: 454 with model VAR in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 454: VAR\n",
            "Model Number: 455 with model VECM in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 455: VECM\n",
            "Model Number: 456 with model VECM in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 456: VECM\n",
            "Model Number: 457 with model VECM in generation 7 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 457: VECM\n",
            "Model Number: 458 with model VECM in generation 7 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 458: VECM\n",
            "Model Number: 459 with model UnivariateRegression in generation 7 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 459: UnivariateRegression\n",
            "Model Number: 460 with model UnivariateRegression in generation 7 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 460: UnivariateRegression\n",
            "Model Number: 461 with model UnivariateRegression in generation 7 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 461: UnivariateRegression\n",
            "Model Number: 462 with model UnivariateRegression in generation 7 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 462: UnivariateRegression\n",
            "New Generation: 8 of 20\n",
            "Model Number: 463 with model FBProphet in generation 8 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.04809\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       214.774   5.99162e-08       105.957    0.001556           1      129   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     138       214.774   4.78644e-09       93.3332      0.5134      0.5134      183   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 464 with model FBProphet in generation 8 of 20\n",
            "Initial log joint probability = -7.27654\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      73       214.016    0.00213042       85.5151   2.329e-05       0.001      153  LS failed, Hessian reset \n",
            "      99        214.14   2.10358e-05       77.0778           1           1      186   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       214.605    0.00275357       79.9621           1           1      314   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       214.794    7.8559e-05       75.2911   9.549e-07       0.001      395  LS failed, Hessian reset \n",
            "     299       215.019    2.3425e-06        63.557     0.03186           1      478   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     338        215.02   2.57932e-08       49.5883      0.7491      0.7491      531   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 465 with model FBProphet in generation 8 of 20\n",
            "Initial log joint probability = -9.18977\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.683   1.93021e-07       96.6408      0.5999      0.5999      127   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     115       212.725    0.00024569       104.512   2.629e-06       0.001      191  LS failed, Hessian reset \n",
            "     146       212.738   5.03655e-09       94.8842       0.155       0.155      232   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 466 with model DatepartRegression in generation 8 of 20\n",
            "Model Number: 467 with model DatepartRegression in generation 8 of 20\n",
            "Model Number: 468 with model DatepartRegression in generation 8 of 20\n",
            "Epoch 1/500\n",
            "6/6 [==============================] - 5s 7ms/step - loss: nan\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0139\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0112\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: -0.0133\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: -0.0105\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: -0.0061\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: -0.0061\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: nan\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: nan\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: nan\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: nan\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: nan\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: nan\n",
            "Model Number: 469 with model RollingRegression in generation 8 of 20\n",
            "Model Number: 470 with model RollingRegression in generation 8 of 20\n",
            "Model Number: 471 with model RollingRegression in generation 8 of 20\n",
            "Model Number: 472 with model RollingRegression in generation 8 of 20\n",
            "Model Number: 473 with model LastValueNaive in generation 8 of 20\n",
            "Model Number: 474 with model LastValueNaive in generation 8 of 20\n",
            "Model Number: 475 with model SeasonalNaive in generation 8 of 20\n",
            "Model Number: 476 with model SeasonalNaive in generation 8 of 20\n",
            "Model Number: 477 with model SeasonalNaive in generation 8 of 20\n",
            "Model Number: 478 with model SeasonalNaive in generation 8 of 20\n",
            "Model Number: 479 with model WindowRegression in generation 8 of 20\n",
            "Model Number: 480 with model WindowRegression in generation 8 of 20\n",
            "Model Number: 481 with model WindowRegression in generation 8 of 20\n",
            "Model Number: 482 with model WindowRegression in generation 8 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 482: WindowRegression\n",
            "Model Number: 483 with model UnobservedComponents in generation 8 of 20\n",
            "Model Number: 484 with model UnobservedComponents in generation 8 of 20\n",
            "Model Number: 485 with model UnobservedComponents in generation 8 of 20\n",
            "Model Number: 486 with model GLS in generation 8 of 20\n",
            "Model Number: 487 with model GLS in generation 8 of 20\n",
            "Model Number: 488 with model GLM in generation 8 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 488: GLM\n",
            "Model Number: 489 with model GLM in generation 8 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 490 with model GLM in generation 8 of 20\n",
            "Model Number: 491 with model GLM in generation 8 of 20\n",
            "Model Number: 492 with model ETS in generation 8 of 20\n",
            "Model Number: 493 with model ETS in generation 8 of 20\n",
            "Model Number: 494 with model ETS in generation 8 of 20\n",
            "Model Number: 495 with model ETS in generation 8 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 495: ETS\n",
            "Model Number: 496 with model AverageValueNaive in generation 8 of 20\n",
            "Model Number: 497 with model AverageValueNaive in generation 8 of 20\n",
            "Model Number: 498 with model ZeroesNaive in generation 8 of 20\n",
            "Model Number: 499 with model ZeroesNaive in generation 8 of 20\n",
            "Model Number: 500 with model ZeroesNaive in generation 8 of 20\n",
            "Model Number: 501 with model GluonTS in generation 8 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 501: GluonTS\n",
            "Model Number: 502 with model GluonTS in generation 8 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 502: GluonTS\n",
            "Model Number: 503 with model GluonTS in generation 8 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 503: GluonTS\n",
            "Model Number: 504 with model GluonTS in generation 8 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 504: GluonTS\n",
            "Model Number: 505 with model VAR in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 505: VAR\n",
            "Model Number: 506 with model VAR in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 506: VAR\n",
            "Model Number: 507 with model VAR in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 507: VAR\n",
            "Model Number: 508 with model VAR in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 508: VAR\n",
            "Model Number: 509 with model VECM in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 509: VECM\n",
            "Model Number: 510 with model VECM in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 510: VECM\n",
            "Model Number: 511 with model VECM in generation 8 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 511: VECM\n",
            "Model Number: 512 with model VECM in generation 8 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 512: VECM\n",
            "Model Number: 513 with model UnivariateRegression in generation 8 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 513: UnivariateRegression\n",
            "Model Number: 514 with model UnivariateRegression in generation 8 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 514: UnivariateRegression\n",
            "Model Number: 515 with model UnivariateRegression in generation 8 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 515: UnivariateRegression\n",
            "Model Number: 516 with model UnivariateRegression in generation 8 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 516: UnivariateRegression\n",
            "New Generation: 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 517 with model FBProphet in generation 9 of 20\n",
            "Initial log joint probability = -5.53497\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      77       318.396     0.0171826       87.2644   0.0002158       0.001      137  LS failed, Hessian reset \n",
            "      99       319.233   5.39713e-05       78.3809      0.2637      0.2637      167   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       319.421    0.00883044        62.755           1           1      305   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       319.598    0.00605868       108.849   8.651e-05       0.001      359  LS failed, Hessian reset \n",
            "     247       319.772   2.07747e-05       69.1347   2.369e-07       0.001      458  LS failed, Hessian reset \n",
            "     282       319.774   6.38521e-09       86.1923      0.2887      0.2887      504   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 518 with model FBProphet in generation 9 of 20\n",
            "Initial log joint probability = -6.63409\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       227.221   3.60513e-06       89.4894      0.1357      0.4283      129   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     163       227.222     7.955e-09       99.2852      0.2545      0.2545      209   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 519 with model FBProphet in generation 9 of 20\n",
            "Initial log joint probability = -9.31981\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       370.121    0.00218681       110.754           1           1      129   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     111        371.91    0.00191422       163.189   1.735e-05       0.001      179  LS failed, Hessian reset \n",
            "     199       373.496   6.31954e-06         97.09      0.3977      0.3977      290   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     286       374.638    0.00253014        174.15   2.927e-05       0.001      480  LS failed, Hessian reset \n",
            "     299       375.241    0.00481299       71.4587       0.757       0.757      494   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     386       375.659   0.000257103       76.6193   3.078e-06       0.001      641  LS failed, Hessian reset \n",
            "     399       375.668    2.2208e-05       80.2634           1           1      657   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     433       375.669   6.27886e-09       63.8536      0.3177      0.3177      703   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 520 with model DatepartRegression in generation 9 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 521 with model DatepartRegression in generation 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 522 with model DatepartRegression in generation 9 of 20\n",
            "Model Number: 523 with model RollingRegression in generation 9 of 20\n",
            "Template Eval Error: LightGBMError('[gamma]: at least one target label is negative') in model 523: RollingRegression\n",
            "Model Number: 524 with model RollingRegression in generation 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Fatal] [gamma]: at least one target label is negative\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 525 with model RollingRegression in generation 9 of 20\n",
            "Model Number: 526 with model RollingRegression in generation 9 of 20\n",
            "Model Number: 527 with model LastValueNaive in generation 9 of 20\n",
            "Model Number: 528 with model LastValueNaive in generation 9 of 20\n",
            "Model Number: 529 with model LastValueNaive in generation 9 of 20\n",
            "Model Number: 530 with model SeasonalNaive in generation 9 of 20\n",
            "Model Number: 531 with model SeasonalNaive in generation 9 of 20\n",
            "Model Number: 532 with model SeasonalNaive in generation 9 of 20\n",
            "Model Number: 533 with model SeasonalNaive in generation 9 of 20\n",
            "Model Number: 534 with model WindowRegression in generation 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/minpack.py:808: OptimizeWarning:\n",
            "\n",
            "Covariance of the parameters could not be estimated\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 534: WindowRegression\n",
            "Model Number: 535 with model WindowRegression in generation 9 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 535: WindowRegression\n",
            "Model Number: 536 with model WindowRegression in generation 9 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 536: WindowRegression\n",
            "Model Number: 537 with model WindowRegression in generation 9 of 20\n",
            "Model Number: 538 with model AverageValueNaive in generation 9 of 20\n",
            "Model Number: 539 with model AverageValueNaive in generation 9 of 20\n",
            "Model Number: 540 with model AverageValueNaive in generation 9 of 20\n",
            "Model Number: 541 with model UnobservedComponents in generation 9 of 20\n",
            "Model Number: 542 with model UnobservedComponents in generation 9 of 20\n",
            "Model Number: 543 with model UnobservedComponents in generation 9 of 20\n",
            "Model Number: 544 with model GLS in generation 9 of 20\n",
            "Model Number: 545 with model GLS in generation 9 of 20\n",
            "Model Number: 546 with model GLS in generation 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/generalized_linear_model.py:774: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 547 with model GLM in generation 9 of 20\n",
            "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 547: GLM\n",
            "Model Number: 548 with model GLM in generation 9 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 548: GLM\n",
            "Model Number: 549 with model GLM in generation 9 of 20\n",
            "Model Number: 550 with model GLM in generation 9 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 551 with model ETS in generation 9 of 20\n",
            "Model Number: 552 with model ETS in generation 9 of 20\n",
            "Model Number: 553 with model ETS in generation 9 of 20\n",
            "Model Number: 554 with model ETS in generation 9 of 20\n",
            "Model Number: 555 with model ZeroesNaive in generation 9 of 20\n",
            "Model Number: 556 with model ZeroesNaive in generation 9 of 20\n",
            "Model Number: 557 with model ZeroesNaive in generation 9 of 20\n",
            "Model Number: 558 with model GluonTS in generation 9 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 558: GluonTS\n",
            "Model Number: 559 with model GluonTS in generation 9 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 559: GluonTS\n",
            "Model Number: 560 with model GluonTS in generation 9 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 560: GluonTS\n",
            "Model Number: 561 with model GluonTS in generation 9 of 20\n",
            "Template Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 561: GluonTS\n",
            "Model Number: 562 with model VAR in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 562: VAR\n",
            "Model Number: 563 with model VAR in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 563: VAR\n",
            "Model Number: 564 with model VAR in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 564: VAR\n",
            "Model Number: 565 with model VAR in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VAR') in model 565: VAR\n",
            "Model Number: 566 with model VECM in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 566: VECM\n",
            "Model Number: 567 with model VECM in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 567: VECM\n",
            "Model Number: 568 with model VECM in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 568: VECM\n",
            "Model Number: 569 with model VECM in generation 9 of 20\n",
            "Template Eval Error: ValueError('Only gave one variable to VECM') in model 569: VECM\n",
            "Model Number: 570 with model UnivariateRegression in generation 9 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 570: UnivariateRegression\n",
            "Model Number: 571 with model UnivariateRegression in generation 9 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 571: UnivariateRegression\n",
            "Model Number: 572 with model UnivariateRegression in generation 9 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 572: UnivariateRegression\n",
            "Model Number: 573 with model UnivariateRegression in generation 9 of 20\n",
            "Template Eval Error: TypeError(\"'>' not supported between instances of 'NoneType' and 'int'\") in model 573: UnivariateRegression\n",
            "New Generation: 10 of 20\n",
            "Model Number: 574 with model FBProphet in generation 10 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.6845\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       211.149   5.92157e-05       83.0099      0.2693           1      127   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     139       211.153   1.99284e-06       80.3835   2.178e-08       0.001      219  LS failed, Hessian reset \n",
            "     168       211.153   8.62871e-09       81.3657      0.3885      0.3885      253   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 575 with model FBProphet in generation 10 of 20\n",
            "Initial log joint probability = -7.28713\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       214.757   0.000554849       72.8941      0.5966      0.5966      130   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     131       215.062    0.00281158       79.2561   2.748e-05       0.001      207  LS failed, Hessian reset \n",
            "     199       215.176   9.87276e-06       85.2163           1           1      295   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     239       215.178   9.50145e-09       72.6183           1           1      351   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 576 with model FBProphet in generation 10 of 20\n",
            "Initial log joint probability = -8.71309\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      72       185.923    0.00149321       79.0647   2.273e-05       0.001      128  LS failed, Hessian reset \n",
            "      99       185.974   4.61095e-07       65.2615      0.2714      0.2714      164   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     127       185.974   1.52771e-06       72.3783   1.935e-08       0.001      236  LS failed, Hessian reset \n",
            "     164       185.975   7.81921e-07       72.6332   1.112e-08       0.001      318  LS failed, Hessian reset \n",
            "     180       185.975   2.86964e-09       63.2149      0.2049      0.2049      341   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 577 with model FBProphet in generation 10 of 20\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 578 with model DatepartRegression in generation 10 of 20\n",
            "Model Number: 579 with model DatepartRegression in generation 10 of 20\n",
            "Model Number: 580 with model DatepartRegression in generation 10 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 581 with model RollingRegression in generation 10 of 20\n",
            "Model Number: 582 with model RollingRegression in generation 10 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 583 with model RollingRegression in generation 10 of 20\n",
            "Model Number: 584 with model RollingRegression in generation 10 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 585 with model LastValueNaive in generation 10 of 20\n",
            "Model Number: 586 with model LastValueNaive in generation 10 of 20\n",
            "Model Number: 587 with model LastValueNaive in generation 10 of 20\n",
            "Model Number: 588 with model SeasonalNaive in generation 10 of 20\n",
            "Model Number: 589 with model SeasonalNaive in generation 10 of 20\n",
            "Model Number: 590 with model SeasonalNaive in generation 10 of 20\n",
            "Model Number: 591 with model SeasonalNaive in generation 10 of 20\n",
            "Model Number: 592 with model WindowRegression in generation 10 of 20\n",
            "Model Number: 593 with model WindowRegression in generation 10 of 20\n",
            "Model Number: 594 with model WindowRegression in generation 10 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 594: WindowRegression\n",
            "Model Number: 595 with model WindowRegression in generation 10 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 595: WindowRegression\n",
            "Model Number: 596 with model AverageValueNaive in generation 10 of 20\n",
            "Model Number: 597 with model AverageValueNaive in generation 10 of 20\n",
            "Model Number: 598 with model UnobservedComponents in generation 10 of 20\n",
            "Model Number: 599 with model UnobservedComponents in generation 10 of 20\n",
            "Model Number: 600 with model UnobservedComponents in generation 10 of 20\n",
            "Model Number: 601 with model GLS in generation 10 of 20\n",
            "Model Number: 602 with model GLS in generation 10 of 20\n",
            "Model Number: 603 with model GLS in generation 10 of 20\n",
            "Model Number: 604 with model GLM in generation 10 of 20\n",
            "Model Number: 605 with model GLM in generation 10 of 20\n",
            "Model Number: 606 with model GLM in generation 10 of 20\n",
            "Model Number: 607 with model GLM in generation 10 of 20\n",
            "Model Number: 608 with model ETS in generation 10 of 20\n",
            "Model Number: 609 with model ETS in generation 10 of 20\n",
            "Model Number: 610 with model ETS in generation 10 of 20\n",
            "Model Number: 611 with model ETS in generation 10 of 20\n",
            "New Generation: 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 612 with model FBProphet in generation 11 of 20\n",
            "Initial log joint probability = -7.27654\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      73       214.016    0.00213042       85.5151   2.329e-05       0.001      153  LS failed, Hessian reset \n",
            "      99        214.14   2.10358e-05       77.0778           1           1      186   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       214.605    0.00275357       79.9621           1           1      314   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       214.794    7.8559e-05       75.2911   9.549e-07       0.001      395  LS failed, Hessian reset \n",
            "     299       215.019    2.3425e-06        63.557     0.03186           1      478   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     338        215.02   2.57932e-08       49.5883      0.7491      0.7491      531   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 613 with model FBProphet in generation 11 of 20\n",
            "Initial log joint probability = -6.74774\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      91       272.256   0.000704979       83.5834   8.139e-06       0.001      164  LS failed, Hessian reset \n",
            "      99       272.306   3.11624e-05       86.0317        0.35        0.35      172   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       272.885   5.41038e-07       71.5292           1           1      295   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     206       272.885   2.77611e-06       90.3544   3.649e-08       0.001      339  LS failed, Hessian reset \n",
            "     231       272.886   6.20324e-09        86.123      0.4839      0.4839      376   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 614 with model FBProphet in generation 11 of 20\n",
            "Initial log joint probability = -15.476\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       142.486   3.51663e-08       96.6999      0.2188      0.7635      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     100       142.486   6.58139e-09       96.1638      0.1754      0.1754      133   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 615 with model FBProphet in generation 11 of 20\n",
            "Initial log joint probability = -7.27654\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      73       214.016    0.00213042       85.5151   2.329e-05       0.001      153  LS failed, Hessian reset \n",
            "      99        214.14   2.10358e-05       77.0778           1           1      186   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       214.605    0.00275357       79.9621           1           1      314   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       214.794    7.8559e-05       75.2911   9.549e-07       0.001      395  LS failed, Hessian reset \n",
            "     299       215.019    2.3425e-06        63.557     0.03186           1      478   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     338        215.02   2.57932e-08       49.5883      0.7491      0.7491      531   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n",
            "Model Number: 616 with model DatepartRegression in generation 11 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 617 with model DatepartRegression in generation 11 of 20\n",
            "Model Number: 618 with model DatepartRegression in generation 11 of 20\n",
            "Model Number: 619 with model RollingRegression in generation 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 619: RollingRegression\n",
            "Model Number: 620 with model RollingRegression in generation 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    6.5s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 621 with model RollingRegression in generation 11 of 20\n",
            "Model Number: 622 with model RollingRegression in generation 11 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 622: RollingRegression\n",
            "Model Number: 623 with model LastValueNaive in generation 11 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 623: LastValueNaive\n",
            "Model Number: 624 with model LastValueNaive in generation 11 of 20\n",
            "Model Number: 625 with model LastValueNaive in generation 11 of 20\n",
            "Model Number: 626 with model SeasonalNaive in generation 11 of 20\n",
            "Model Number: 627 with model SeasonalNaive in generation 11 of 20\n",
            "Model Number: 628 with model SeasonalNaive in generation 11 of 20\n",
            "Model Number: 629 with model SeasonalNaive in generation 11 of 20\n",
            "Model Number: 630 with model WindowRegression in generation 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 630: WindowRegression\n",
            "Model Number: 631 with model WindowRegression in generation 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 632 with model WindowRegression in generation 11 of 20\n",
            "Model Number: 633 with model WindowRegression in generation 11 of 20\n",
            "Model Number: 634 with model AverageValueNaive in generation 11 of 20\n",
            "Model Number: 635 with model AverageValueNaive in generation 11 of 20\n",
            "Model Number: 636 with model AverageValueNaive in generation 11 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 636: AverageValueNaive\n",
            "Model Number: 637 with model ETS in generation 11 of 20\n",
            "Model Number: 638 with model ETS in generation 11 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters.py:712: ConvergenceWarning:\n",
            "\n",
            "Optimization failed to converge. Check mle_retvals.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 639 with model ETS in generation 11 of 20\n",
            "Model Number: 640 with model ETS in generation 11 of 20\n",
            "Model Number: 641 with model UnobservedComponents in generation 11 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 641: UnobservedComponents\n",
            "Model Number: 642 with model UnobservedComponents in generation 11 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 642: UnobservedComponents\n",
            "Model Number: 643 with model UnobservedComponents in generation 11 of 20\n",
            "Model Number: 644 with model GLS in generation 11 of 20\n",
            "Model Number: 645 with model GLS in generation 11 of 20\n",
            "Model Number: 646 with model GLS in generation 11 of 20\n",
            "Model Number: 647 with model GLM in generation 11 of 20\n",
            "Model Number: 648 with model GLM in generation 11 of 20\n",
            "Model Number: 649 with model GLM in generation 11 of 20\n",
            "Model Number: 650 with model GLM in generation 11 of 20\n",
            "New Generation: 12 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 651 with model FBProphet in generation 12 of 20\n",
            "Initial log joint probability = -4.98414\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      72       314.746    0.00428839       104.948   4.611e-05       0.001      118  LS failed, Hessian reset \n",
            "      99       315.066   0.000300416        74.571       6.929      0.6929      152   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     145       315.085   0.000112918       79.9414   1.241e-06       0.001      249  LS failed, Hessian reset \n",
            "     192        315.09   1.70588e-09       67.4215     0.06647     0.06647      312   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 652 with model FBProphet in generation 12 of 20\n",
            "Initial log joint probability = -6.24788\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       274.025   0.000739148       82.7527      0.5098      0.5098      123   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     110       274.259    0.00138693       76.2923   1.346e-05       0.001      180  LS failed, Hessian reset \n",
            "     174       274.587   2.44445e-05       75.1551   3.201e-07       0.001      296  LS failed, Hessian reset \n",
            "     199       274.588   4.03265e-08       69.4608      0.8984      0.8984      329   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     239       274.588   8.93075e-09       62.4038      0.6747      0.6747      386   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 653 with model FBProphet in generation 12 of 20\n",
            "Initial log joint probability = -16.1602\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       92.3667   2.06849e-06       84.2797      0.3681      0.3681      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     154       92.3782    8.4234e-05       87.0562    9.25e-07       0.001      237  LS failed, Hessian reset \n",
            "     199       92.3838   1.28631e-07       67.6678           1           1      292   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     224       92.3838   5.38803e-09       95.5213      0.3164      0.3164      326   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 654 with model FBProphet in generation 12 of 20\n",
            "Initial log joint probability = -8.10265\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      61       224.238   0.000161151       46.8561   2.405e-06       0.001      122  LS failed, Hessian reset \n",
            "      96       224.255   8.50123e-05       61.9565   1.527e-06       0.001      215  LS failed, Hessian reset \n",
            "      99       224.257   2.36854e-05       50.1181      0.9195      0.9195      218   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     123       224.259   1.14779e-05       38.8251   1.663e-07       0.001      307  LS failed, Hessian reset \n",
            "     154       224.259   2.16347e-06       80.3858   3.117e-08       0.001      382  LS failed, Hessian reset \n",
            "     164       224.259   5.41498e-09       47.1236     0.09712     0.09712      401   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 655 with model DatepartRegression in generation 12 of 20\n",
            "Model Number: 656 with model DatepartRegression in generation 12 of 20\n",
            "Model Number: 657 with model DatepartRegression in generation 12 of 20\n",
            "Model Number: 658 with model RollingRegression in generation 12 of 20\n",
            "Model Number: 659 with model RollingRegression in generation 12 of 20\n",
            "Model Number: 660 with model RollingRegression in generation 12 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 661 with model RollingRegression in generation 12 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 662 with model ETS in generation 12 of 20\n",
            "Model Number: 663 with model ETS in generation 12 of 20\n",
            "Model Number: 664 with model ETS in generation 12 of 20\n",
            "Model Number: 665 with model ETS in generation 12 of 20\n",
            "Model Number: 666 with model LastValueNaive in generation 12 of 20\n",
            "Model Number: 667 with model SeasonalNaive in generation 12 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 667: SeasonalNaive\n",
            "Model Number: 668 with model SeasonalNaive in generation 12 of 20\n",
            "Model Number: 669 with model SeasonalNaive in generation 12 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 669: SeasonalNaive\n",
            "Model Number: 670 with model SeasonalNaive in generation 12 of 20\n",
            "Model Number: 671 with model WindowRegression in generation 12 of 20\n",
            "Epoch 1/100\n",
            "6/6 [==============================] - 5s 7ms/step - loss: 98.8732\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 95.9033\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 92.1075\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 87.0532\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 78.9108\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 67.5048\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 57.1532\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 52.6088\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 47.3465\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 38.2847\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 31.5952\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 27.9163\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 28.8376\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 29.1067\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 28.3255\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 27.4486\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 27.3730\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.8696\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.8482\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 27.7147\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 28.0705\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 27.9820\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.4035\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.7261\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 27.2643\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.6387\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.6576\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 27.2013\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.2676\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.1587\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.9369\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.2514\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.3665\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 27.4264\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 26.5981\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.5516\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.2869\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.9310\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.4627\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.6714\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.0948\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 26.3835\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.0340\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.5770\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.6838\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.4848\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.9468\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.1471\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.3282\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.3144\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.7266\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 27.1676\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.4598\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.2187\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 25.8223\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.9291\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 25.6136\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.9789\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 25.5810\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.0923\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 25.7340\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.7754\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 25.5278\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 26.0570\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.7043\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.3637\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.3856\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 26.0707\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.2511\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.0813\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.3614\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.8365\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.7451\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.0431\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 25.0768\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.9294\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 25.2803\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.2396\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.9468\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.2104\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.7854\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 24.8038\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.5248\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.1593\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.9004\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.2486\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.6288\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.0498\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.3931\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.3746\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 23.9567\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.2140\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 23.8037\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.1171\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.2211\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.7306\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.2229\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 24.2804\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 24.1814\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 23.6450\n",
            "Model Number: 672 with model WindowRegression in generation 12 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 672: WindowRegression\n",
            "Model Number: 673 with model WindowRegression in generation 12 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 673: WindowRegression\n",
            "Model Number: 674 with model WindowRegression in generation 12 of 20\n",
            "Model Number: 675 with model AverageValueNaive in generation 12 of 20\n",
            "Model Number: 676 with model AverageValueNaive in generation 12 of 20\n",
            "Model Number: 677 with model AverageValueNaive in generation 12 of 20\n",
            "Model Number: 678 with model UnobservedComponents in generation 12 of 20\n",
            "Model Number: 679 with model UnobservedComponents in generation 12 of 20\n",
            "Model Number: 680 with model UnobservedComponents in generation 12 of 20\n",
            "Model Number: 681 with model GLS in generation 12 of 20\n",
            "Model Number: 682 with model GLS in generation 12 of 20\n",
            "Model Number: 683 with model GLS in generation 12 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 683: GLS\n",
            "Model Number: 684 with model GLM in generation 12 of 20\n",
            "Model Number: 685 with model GLM in generation 12 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 685: GLM\n",
            "Model Number: 686 with model GLM in generation 12 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 686: GLM\n",
            "Model Number: 687 with model GLM in generation 12 of 20\n",
            "New Generation: 13 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/generalized_linear_model.py:273: DomainWarning:\n",
            "\n",
            "The inverse_power link function does not respect the domain of the Gamma family.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 688 with model FBProphet in generation 13 of 20\n",
            "Initial log joint probability = -8.76853\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       217.351    3.0274e-05       90.5667       0.452           1      131   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     157       217.354   1.53444e-06       80.2797   1.341e-08       0.001      246  LS failed, Hessian reset \n",
            "     178       217.354   8.88532e-09       93.1748      0.6416      0.6416      268   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 689 with model FBProphet in generation 13 of 20\n",
            "Initial log joint probability = -11.0879\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       214.715   1.30577e-06       95.3018      0.4036      0.4036      121   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     117       214.715    3.4312e-09       90.8181      0.1259      0.1259      147   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 690 with model FBProphet in generation 13 of 20\n",
            "Initial log joint probability = -8.76853\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       217.351    3.0274e-05       90.5667       0.452           1      131   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     157       217.354   1.53444e-06       80.2797   1.341e-08       0.001      246  LS failed, Hessian reset \n",
            "     178       217.354   8.88532e-09       93.1748      0.6416      0.6416      268   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 691 with model DatepartRegression in generation 13 of 20\n",
            "Model Number: 692 with model DatepartRegression in generation 13 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 693 with model DatepartRegression in generation 13 of 20\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.004503\n",
            "nSV = 190\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.004827\n",
            "nSV = 190\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.005644\n",
            "nSV = 190\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.005462\n",
            "nSV = 190\n",
            "[LibLinear]......................................................................................................................................................\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.005814\n",
            "nSV = 190\n",
            "[LibLinear]........................................................................................................................................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning:\n",
            "\n",
            "Liblinear failed to converge, increase the number of iterations.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".............\n",
            "optimization finished, #iter = 1500\n",
            "\n",
            "WARNING: reaching max number of iterations\n",
            "Using -s 11 may be faster\n",
            "\n",
            "Objective value = -0.006169\n",
            "nSV = 190\n",
            "Model Number: 694 with model RollingRegression in generation 13 of 20\n",
            "Model Number: 695 with model RollingRegression in generation 13 of 20\n",
            "Epoch 1/50\n",
            "6/6 [==============================] - 5s 6ms/step - loss: 0.9862\n",
            "Epoch 2/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.9443\n",
            "Epoch 3/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.8341\n",
            "Epoch 4/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.6595\n",
            "Epoch 5/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4574\n",
            "Epoch 6/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3018\n",
            "Epoch 7/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1997\n",
            "Epoch 8/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1254\n",
            "Epoch 9/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0685\n",
            "Epoch 10/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0346\n",
            "Epoch 11/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0217\n",
            "Epoch 12/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0164\n",
            "Epoch 13/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0194\n",
            "Epoch 14/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0160\n",
            "Epoch 15/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0154\n",
            "Epoch 16/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0133\n",
            "Epoch 17/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0162\n",
            "Epoch 18/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0182\n",
            "Epoch 19/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0162\n",
            "Epoch 20/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0160\n",
            "Epoch 21/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0138\n",
            "Epoch 22/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0173\n",
            "Epoch 23/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0121\n",
            "Epoch 24/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0128\n",
            "Epoch 25/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0134\n",
            "Epoch 26/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0095\n",
            "Epoch 27/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0172\n",
            "Epoch 28/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0157\n",
            "Epoch 29/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0131\n",
            "Epoch 30/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0123\n",
            "Epoch 31/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0117\n",
            "Epoch 32/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0095\n",
            "Epoch 33/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0109\n",
            "Epoch 34/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0149\n",
            "Epoch 35/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0098\n",
            "Epoch 36/50\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0091\n",
            "Epoch 37/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0103\n",
            "Epoch 38/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0098\n",
            "Epoch 39/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0116\n",
            "Epoch 40/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0092\n",
            "Epoch 41/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0094\n",
            "Epoch 42/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0098\n",
            "Epoch 43/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0090\n",
            "Epoch 44/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0116\n",
            "Epoch 45/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0086\n",
            "Epoch 46/50\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0066\n",
            "Epoch 47/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0111\n",
            "Epoch 48/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0092\n",
            "Epoch 49/50\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0093\n",
            "Epoch 50/50\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0079\n",
            "Model Number: 696 with model RollingRegression in generation 13 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 697 with model RollingRegression in generation 13 of 20\n",
            "Model Number: 698 with model ETS in generation 13 of 20\n",
            "Model Number: 699 with model ETS in generation 13 of 20\n",
            "Model Number: 700 with model ETS in generation 13 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters.py:924: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/holtwinters.py:930: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 701 with model ETS in generation 13 of 20\n",
            "Model Number: 702 with model LastValueNaive in generation 13 of 20\n",
            "Model Number: 703 with model LastValueNaive in generation 13 of 20\n",
            "Model Number: 704 with model LastValueNaive in generation 13 of 20\n",
            "Model Number: 705 with model SeasonalNaive in generation 13 of 20\n",
            "Model Number: 706 with model SeasonalNaive in generation 13 of 20\n",
            "Model Number: 707 with model SeasonalNaive in generation 13 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 707: SeasonalNaive\n",
            "Model Number: 708 with model SeasonalNaive in generation 13 of 20\n",
            "Model Number: 709 with model WindowRegression in generation 13 of 20\n",
            "Model Number: 710 with model WindowRegression in generation 13 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 710: WindowRegression\n",
            "Model Number: 711 with model WindowRegression in generation 13 of 20\n",
            "FillNA method not known, returning original\n",
            "Model Number: 712 with model WindowRegression in generation 13 of 20\n",
            "Model Number: 713 with model AverageValueNaive in generation 13 of 20\n",
            "Model Number: 714 with model AverageValueNaive in generation 13 of 20\n",
            "Model Number: 715 with model AverageValueNaive in generation 13 of 20\n",
            "Model Number: 716 with model UnobservedComponents in generation 13 of 20\n",
            "Model Number: 717 with model UnobservedComponents in generation 13 of 20\n",
            "Model Number: 718 with model UnobservedComponents in generation 13 of 20\n",
            "Model Number: 719 with model GLS in generation 13 of 20\n",
            "Model Number: 720 with model GLS in generation 13 of 20\n",
            "Model Number: 721 with model GLS in generation 13 of 20\n",
            "Model Number: 722 with model GLM in generation 13 of 20\n",
            "Model Number: 723 with model GLM in generation 13 of 20\n",
            "Model Number: 724 with model GLM in generation 13 of 20\n",
            "Model Number: 725 with model GLM in generation 13 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 725: GLM\n",
            "New Generation: 14 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 726 with model FBProphet in generation 14 of 20\n",
            "Initial log joint probability = -3.83308\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       323.633   6.68422e-06       66.7722      0.7493      0.7493      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     108       323.637   3.04822e-05       84.5772   3.641e-07       0.001      185  LS failed, Hessian reset \n",
            "     149       323.639   8.27332e-07       81.3447   1.022e-08       0.001      277  LS failed, Hessian reset \n",
            "     162       323.639   7.40259e-09       63.8903     0.06768           1      296   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 727 with model FBProphet in generation 14 of 20\n",
            "Initial log joint probability = -8.53618\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       197.803    0.00010036       81.7224           1           1      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       199.689   3.42246e-06       94.3637      0.4927      0.4927      256   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       199.689   8.46302e-09       84.9531      0.5035      0.5035      295   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 728 with model FBProphet in generation 14 of 20\n",
            "Initial log joint probability = -3.74676\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      49       336.729    0.00196061       73.9632   2.524e-05       0.001       97  LS failed, Hessian reset \n",
            "      99       336.902   2.60651e-05       58.9949      0.2308           1      169   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       336.992   6.73386e-07        63.455      0.2936           1      306   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     277       337.002   4.37287e-08       66.9855   5.968e-10       0.001      461  LS failed, Hessian reset \n",
            "     278       337.002   1.15022e-08       62.4173      0.3669           1      463   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n",
            "Model Number: 729 with model DatepartRegression in generation 14 of 20\n",
            "Model Number: 730 with model DatepartRegression in generation 14 of 20\n",
            "Model Number: 731 with model DatepartRegression in generation 14 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 731: DatepartRegression\n",
            "Model Number: 732 with model RollingRegression in generation 14 of 20\n",
            "Model Number: 733 with model RollingRegression in generation 14 of 20\n",
            "Model Number: 734 with model RollingRegression in generation 14 of 20\n",
            "Model Number: 735 with model RollingRegression in generation 14 of 20\n",
            "Model Number: 736 with model ETS in generation 14 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 736: ETS\n",
            "Model Number: 737 with model ETS in generation 14 of 20\n",
            "Model Number: 738 with model ETS in generation 14 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 738: ETS\n",
            "Model Number: 739 with model ETS in generation 14 of 20\n",
            "Model Number: 740 with model LastValueNaive in generation 14 of 20\n",
            "Model Number: 741 with model LastValueNaive in generation 14 of 20\n",
            "Model Number: 742 with model SeasonalNaive in generation 14 of 20\n",
            "Model Number: 743 with model SeasonalNaive in generation 14 of 20\n",
            "Model Number: 744 with model SeasonalNaive in generation 14 of 20\n",
            "Model Number: 745 with model SeasonalNaive in generation 14 of 20\n",
            "Model Number: 746 with model WindowRegression in generation 14 of 20\n",
            "Model Number: 747 with model WindowRegression in generation 14 of 20\n",
            "FillNA method not known, returning original\n",
            "Model Number: 748 with model WindowRegression in generation 14 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 749 with model WindowRegression in generation 14 of 20\n",
            "Model Number: 750 with model AverageValueNaive in generation 14 of 20\n",
            "Model Number: 751 with model AverageValueNaive in generation 14 of 20\n",
            "Model Number: 752 with model AverageValueNaive in generation 14 of 20\n",
            "Model Number: 753 with model UnobservedComponents in generation 14 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 753: UnobservedComponents\n",
            "Model Number: 754 with model UnobservedComponents in generation 14 of 20\n",
            "Model Number: 755 with model UnobservedComponents in generation 14 of 20\n",
            "Model Number: 756 with model GLS in generation 14 of 20\n",
            "Model Number: 757 with model GLS in generation 14 of 20\n",
            "Model Number: 758 with model GLS in generation 14 of 20\n",
            "Model Number: 759 with model GLM in generation 14 of 20\n",
            "Model Number: 760 with model GLM in generation 14 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 761 with model GLM in generation 14 of 20\n",
            "Model Number: 762 with model GLM in generation 14 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New Generation: 15 of 20\n",
            "Model Number: 763 with model FBProphet in generation 15 of 20\n",
            "Initial log joint probability = -3.80057\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       343.325   3.02324e-07       85.5248        1.71      0.4521      134   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     114       343.325   1.24446e-07       99.6629   1.607e-09       0.001      186  LS failed, Hessian reset \n",
            "     120       343.325    6.8292e-09       80.5353      0.4131      0.4131      194   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 764 with model FBProphet in generation 15 of 20\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 765 with model FBProphet in generation 15 of 20\n",
            "Initial log joint probability = -16.9236\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       141.845   6.82304e-08       101.486           1           1      125   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     103       141.845    9.8086e-08       99.6474   1.003e-09       0.001      172  LS failed, Hessian reset \n",
            "     109       141.845   5.73639e-09       98.8267      0.4911      0.4911      179   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 766 with model DatepartRegression in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 767 with model DatepartRegression in generation 15 of 20\n",
            "Model Number: 768 with model DatepartRegression in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 769 with model RollingRegression in generation 15 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 769: RollingRegression\n",
            "Model Number: 770 with model RollingRegression in generation 15 of 20\n",
            "Model Number: 771 with model RollingRegression in generation 15 of 20\n",
            "Model Number: 772 with model RollingRegression in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 773 with model ETS in generation 15 of 20\n",
            "Model Number: 774 with model ETS in generation 15 of 20\n",
            "Model Number: 775 with model ETS in generation 15 of 20\n",
            "Model Number: 776 with model LastValueNaive in generation 15 of 20\n",
            "Model Number: 777 with model LastValueNaive in generation 15 of 20\n",
            "Model Number: 778 with model SeasonalNaive in generation 15 of 20\n",
            "Model Number: 779 with model SeasonalNaive in generation 15 of 20\n",
            "Model Number: 780 with model SeasonalNaive in generation 15 of 20\n",
            "Model Number: 781 with model SeasonalNaive in generation 15 of 20\n",
            "Model Number: 782 with model WindowRegression in generation 15 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 782: WindowRegression\n",
            "Model Number: 783 with model WindowRegression in generation 15 of 20\n",
            "Model Number: 784 with model WindowRegression in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 785 with model WindowRegression in generation 15 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 785: WindowRegression\n",
            "Model Number: 786 with model AverageValueNaive in generation 15 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 786: AverageValueNaive\n",
            "Model Number: 787 with model AverageValueNaive in generation 15 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 787: AverageValueNaive\n",
            "Model Number: 788 with model AverageValueNaive in generation 15 of 20\n",
            "Model Number: 789 with model UnobservedComponents in generation 15 of 20\n",
            "Model Number: 790 with model UnobservedComponents in generation 15 of 20\n",
            "Model Number: 791 with model UnobservedComponents in generation 15 of 20\n",
            "Model Number: 792 with model GLS in generation 15 of 20\n",
            "Model Number: 793 with model GLS in generation 15 of 20\n",
            "Model Number: 794 with model GLS in generation 15 of 20\n",
            "Model Number: 795 with model GLM in generation 15 of 20\n",
            "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 795: GLM\n",
            "Model Number: 796 with model GLM in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:428: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 797 with model GLM in generation 15 of 20\n",
            "Model Number: 798 with model GLM in generation 15 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New Generation: 16 of 20\n",
            "Model Number: 799 with model FBProphet in generation 16 of 20\n",
            "Initial log joint probability = -2.50039\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       407.542   1.79105e-07       102.342           1           1      138   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     119       407.542   2.29385e-08       95.3495        0.41           1      162   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 800 with model FBProphet in generation 16 of 20\n",
            "Initial log joint probability = -2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99        407.58   2.47682e-06       91.3118      0.4044      0.4044      125   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     174       407.587   8.60901e-09       93.5043       0.512       0.512      216   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 801 with model FBProphet in generation 16 of 20\n",
            "Initial log joint probability = -7.19464\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       219.244   1.02522e-05       75.8724      0.1903      0.1903      131   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     148       219.249    5.7896e-06       86.7845   6.932e-08       0.001      238  LS failed, Hessian reset \n",
            "     182        219.25   9.84176e-09       89.6023      0.4645      0.4645      284   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 802 with model DatepartRegression in generation 16 of 20\n",
            "Model Number: 803 with model DatepartRegression in generation 16 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 804 with model DatepartRegression in generation 16 of 20\n",
            "Model Number: 805 with model RollingRegression in generation 16 of 20\n",
            "Model Number: 806 with model RollingRegression in generation 16 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 807 with model RollingRegression in generation 16 of 20\n",
            "Model Number: 808 with model RollingRegression in generation 16 of 20\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:195: UserWarning:\n",
            "\n",
            "Usage of np.ndarray subset (sliced data) is not recommended due to it will double the peak memory cost in LightGBM.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 809 with model ETS in generation 16 of 20\n",
            "Model Number: 810 with model ETS in generation 16 of 20\n",
            "Model Number: 811 with model ETS in generation 16 of 20\n",
            "Model Number: 812 with model ETS in generation 16 of 20\n",
            "Model Number: 813 with model LastValueNaive in generation 16 of 20\n",
            "Model Number: 814 with model LastValueNaive in generation 16 of 20\n",
            "Model Number: 815 with model LastValueNaive in generation 16 of 20\n",
            "Model Number: 816 with model SeasonalNaive in generation 16 of 20\n",
            "Model Number: 817 with model SeasonalNaive in generation 16 of 20\n",
            "Model Number: 818 with model SeasonalNaive in generation 16 of 20\n",
            "Model Number: 819 with model SeasonalNaive in generation 16 of 20\n",
            "Model Number: 820 with model WindowRegression in generation 16 of 20\n",
            "Model Number: 821 with model WindowRegression in generation 16 of 20\n",
            "Model Number: 822 with model WindowRegression in generation 16 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:434: FutureWarning:\n",
            "\n",
            "The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 823 with model WindowRegression in generation 16 of 20\n",
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 823: WindowRegression\n",
            "Model Number: 824 with model AverageValueNaive in generation 16 of 20\n",
            "Model Number: 825 with model AverageValueNaive in generation 16 of 20\n",
            "Model Number: 826 with model AverageValueNaive in generation 16 of 20\n",
            "Model Number: 827 with model UnobservedComponents in generation 16 of 20\n",
            "Model Number: 828 with model UnobservedComponents in generation 16 of 20\n",
            "Model Number: 829 with model UnobservedComponents in generation 16 of 20\n",
            "Model Number: 830 with model GLS in generation 16 of 20\n",
            "Model Number: 831 with model GLS in generation 16 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 831: GLS\n",
            "Model Number: 832 with model GLM in generation 16 of 20\n",
            "Model Number: 833 with model GLM in generation 16 of 20\n",
            "Model Number: 834 with model GLM in generation 16 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 835 with model GLM in generation 16 of 20\n",
            "New Generation: 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 836 with model FBProphet in generation 17 of 20\n",
            "Initial log joint probability = -5.79709\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       241.366   1.43992e-06       94.8159      0.5732      0.5732      126   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     162       241.455   0.000164498       71.6787   2.018e-06       0.001      247  LS failed, Hessian reset \n",
            "     199       241.532   7.41552e-06       65.2634      0.7701      0.7701      292   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     299       241.565   5.78346e-08       108.808      0.4691      0.4691      420   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     307       241.572   8.61842e-05       86.2771   8.238e-07       0.001      471  LS failed, Hessian reset \n",
            "     374       241.576   6.90071e-08       105.747   7.967e-10       0.001      606  LS failed, Hessian reset \n",
            "     378       241.576   7.14589e-09       85.6219       0.529       0.529      610   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 837 with model FBProphet in generation 17 of 20\n",
            "Initial log joint probability = -7.25584\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       218.526     0.0358019       68.5401           1           1      123   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     195       218.945   2.37351e-05       96.4801   2.894e-07       0.001      292  LS failed, Hessian reset \n",
            "     199       218.945   1.34898e-06       63.3815      0.3062      0.3062      297   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     250       218.946   7.82443e-09       64.3357       0.589       0.589      358   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 838 with model FBProphet in generation 17 of 20\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 839 with model FBProphet in generation 17 of 20\n",
            "Initial log joint probability = -7.25584\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       218.526     0.0358019       68.5401           1           1      123   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     195       218.945   2.37351e-05       96.4801   2.894e-07       0.001      292  LS failed, Hessian reset \n",
            "     199       218.945   1.34898e-06       63.3815      0.3062      0.3062      297   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     250       218.946   7.82443e-09       64.3357       0.589       0.589      358   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 840 with model DatepartRegression in generation 17 of 20\n",
            "Model Number: 841 with model DatepartRegression in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 842 with model DatepartRegression in generation 17 of 20\n",
            "Model Number: 843 with model RollingRegression in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 844 with model RollingRegression in generation 17 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 844: RollingRegression\n",
            "Model Number: 845 with model RollingRegression in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 846 with model RollingRegression in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 847 with model ETS in generation 17 of 20\n",
            "Model Number: 848 with model ETS in generation 17 of 20\n",
            "Model Number: 849 with model ETS in generation 17 of 20\n",
            "Model Number: 850 with model ETS in generation 17 of 20\n",
            "Model Number: 851 with model LastValueNaive in generation 17 of 20\n",
            "Model Number: 852 with model LastValueNaive in generation 17 of 20\n",
            "Model Number: 853 with model AverageValueNaive in generation 17 of 20\n",
            "Model Number: 854 with model AverageValueNaive in generation 17 of 20\n",
            "Model Number: 855 with model AverageValueNaive in generation 17 of 20\n",
            "Model Number: 856 with model SeasonalNaive in generation 17 of 20\n",
            "Model Number: 857 with model SeasonalNaive in generation 17 of 20\n",
            "Model Number: 858 with model SeasonalNaive in generation 17 of 20\n",
            "Model Number: 859 with model SeasonalNaive in generation 17 of 20\n",
            "Model Number: 860 with model WindowRegression in generation 17 of 20\n",
            "Model Number: 861 with model WindowRegression in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 862 with model WindowRegression in generation 17 of 20\n",
            "Model Number: 863 with model WindowRegression in generation 17 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 864 with model UnobservedComponents in generation 17 of 20\n",
            "Model Number: 865 with model UnobservedComponents in generation 17 of 20\n",
            "Model Number: 866 with model UnobservedComponents in generation 17 of 20\n",
            "Model Number: 867 with model GLS in generation 17 of 20\n",
            "Model Number: 868 with model GLS in generation 17 of 20\n",
            "Model Number: 869 with model GLS in generation 17 of 20\n",
            "Model Number: 870 with model GLM in generation 17 of 20\n",
            "Model Number: 871 with model GLM in generation 17 of 20\n",
            "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 871: GLM\n",
            "Model Number: 872 with model GLM in generation 17 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:1440: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in log\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 873 with model GLM in generation 17 of 20\n",
            "New Generation: 18 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:451: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 874 with model FBProphet in generation 18 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -7.50383\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      92       249.183    0.00346985       88.1921   4.451e-05       0.001      153  LS failed, Hessian reset \n",
            "      99       249.372   0.000381127        85.238      0.9137      0.9137      162   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       249.915   0.000933899       63.6387      0.6534      0.6534      282   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     299       250.221   1.99637e-07       58.5796      0.3248      0.9035      429   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     310       250.222   3.18924e-06       78.7703   4.762e-08       0.001      484  LS failed, Hessian reset \n",
            "     357       250.222   5.86414e-08       60.9201   9.392e-10       0.001      591  LS failed, Hessian reset \n",
            "     359       250.222   1.57983e-08       54.8238      0.8781      0.8781      593   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n",
            "Model Number: 875 with model FBProphet in generation 18 of 20\n",
            "Initial log joint probability = -5.76865\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       247.553    3.1858e-07        84.172           1           1      125   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     127       247.553   1.74434e-07       100.154   1.766e-09       0.001      205  LS failed, Hessian reset \n",
            "     135       247.553   4.05128e-09       84.1841       0.258       0.258      215   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 876 with model FBProphet in generation 18 of 20\n",
            "Initial log joint probability = -9.53942\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       268.075     0.0113457       77.3416           1           1      122   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     116       269.216    0.00145591       79.0536   1.839e-05       0.001      178  LS failed, Hessian reset \n",
            "     199       269.846   2.75252e-06       82.8374       0.693       0.693      285   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     253       269.903    4.1162e-09       76.8313      0.3374      0.3374      358   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 877 with model DatepartRegression in generation 18 of 20\n",
            "Model Number: 878 with model DatepartRegression in generation 18 of 20\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Model Number: 879 with model DatepartRegression in generation 18 of 20\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 4s 9ms/step - loss: 0.4509\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.2351\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1370\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1619\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1827\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1683\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1362\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1212\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1217\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1266\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1171\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1169\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1222\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1160\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1060\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0972\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1112\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.1056\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.0993\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1066\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0980\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0962\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1035\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.0901\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0979\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.0961\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.0989\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.0943\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1058\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0977\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0909\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.1061\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0986\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0973\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0927\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.0979\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.1009\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.0965\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.0951\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.0973\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.0897\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.0963\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0920\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0941\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.0924\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0881\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0902\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0906\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0899\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.0830\n",
            "Model Number: 880 with model RollingRegression in generation 18 of 20\n",
            "Model Number: 881 with model RollingRegression in generation 18 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 881: RollingRegression\n",
            "Model Number: 882 with model RollingRegression in generation 18 of 20\n",
            "Model Number: 883 with model RollingRegression in generation 18 of 20\n",
            "Model Number: 884 with model ETS in generation 18 of 20\n",
            "Model Number: 885 with model ETS in generation 18 of 20\n",
            "Model Number: 886 with model ETS in generation 18 of 20\n",
            "Model Number: 887 with model ETS in generation 18 of 20\n",
            "Model Number: 888 with model LastValueNaive in generation 18 of 20\n",
            "Model Number: 889 with model AverageValueNaive in generation 18 of 20\n",
            "Model Number: 890 with model AverageValueNaive in generation 18 of 20\n",
            "Model Number: 891 with model AverageValueNaive in generation 18 of 20\n",
            "Model Number: 892 with model SeasonalNaive in generation 18 of 20\n",
            "Model Number: 893 with model SeasonalNaive in generation 18 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/minpack.py:808: OptimizeWarning:\n",
            "\n",
            "Covariance of the parameters could not be estimated\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 894 with model SeasonalNaive in generation 18 of 20\n",
            "Model Number: 895 with model SeasonalNaive in generation 18 of 20\n",
            "Model Number: 896 with model WindowRegression in generation 18 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 896: WindowRegression\n",
            "Model Number: 897 with model WindowRegression in generation 18 of 20\n",
            "Model Number: 898 with model WindowRegression in generation 18 of 20\n",
            "Model Number: 899 with model WindowRegression in generation 18 of 20\n",
            "Model Number: 900 with model GLS in generation 18 of 20\n",
            "Model Number: 901 with model GLS in generation 18 of 20\n",
            "Model Number: 902 with model UnobservedComponents in generation 18 of 20\n",
            "Model Number: 903 with model UnobservedComponents in generation 18 of 20\n",
            "Model Number: 904 with model UnobservedComponents in generation 18 of 20\n",
            "Model Number: 905 with model GLM in generation 18 of 20\n",
            "Model Number: 906 with model GLM in generation 18 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 906: GLM\n",
            "Model Number: 907 with model GLM in generation 18 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 908 with model GLM in generation 18 of 20\n",
            "New Generation: 19 of 20\n",
            "Model Number: 909 with model FBProphet in generation 19 of 20\n",
            "Initial log joint probability = -7.25839\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       273.261   0.000263462       64.0599      0.5281      0.5281      135   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107       273.312   0.000400316       46.4763   6.903e-06       0.001      192  LS failed, Hessian reset \n",
            "     186       274.572   0.000535969        74.893   7.987e-06       0.001      335  LS failed, Hessian reset \n",
            "     199       274.613   0.000430878       71.2864           1           1      352   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     222       274.633   4.27139e-05       79.2229   8.378e-07       0.001      414  LS failed, Hessian reset \n",
            "     299       274.665   5.60416e-05       87.7213      0.8179      0.8179      512   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     399       274.908   1.83979e-05       64.6778           1           1      648   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     499       275.462   9.29823e-05        71.083      0.1333           1      779   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     599       275.721   3.87929e-06        57.218      0.7211      0.7211      912   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     629       275.722   6.96966e-07       53.7736   1.006e-08       0.001      995  LS failed, Hessian reset \n",
            "     646       275.722   7.34609e-09       55.6855      0.4154      0.4154     1017   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 910 with model FBProphet in generation 19 of 20\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 911 with model FBProphet in generation 19 of 20\n",
            "Initial log joint probability = -14.6449\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      47       97.0093   7.33388e-09       99.8681      0.1738      0.1738       66   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 912 with model FBProphet in generation 19 of 20\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 913 with model DatepartRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 914 with model DatepartRegression in generation 19 of 20\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
            "Template Eval Error: LightGBMError('Check failed: label > 0 at /__w/1/s/python-package/compile/src/metric/regression_metric.hpp, line 268 .\\n') in model 914: DatepartRegression\n",
            "Model Number: 915 with model DatepartRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[LightGBM] [Fatal] Check failed: label > 0 at /__w/1/s/python-package/compile/src/metric/regression_metric.hpp, line 268 .\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 916 with model RollingRegression in generation 19 of 20\n",
            "Model Number: 917 with model RollingRegression in generation 19 of 20\n",
            "Model Number: 918 with model RollingRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 919 with model RollingRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 920 with model ETS in generation 19 of 20\n",
            "Model Number: 921 with model ETS in generation 19 of 20\n",
            "Model Number: 922 with model ETS in generation 19 of 20\n",
            "Model Number: 923 with model ETS in generation 19 of 20\n",
            "Model Number: 924 with model LastValueNaive in generation 19 of 20\n",
            "Model Number: 925 with model LastValueNaive in generation 19 of 20\n",
            "Model Number: 926 with model LastValueNaive in generation 19 of 20\n",
            "Model Number: 927 with model AverageValueNaive in generation 19 of 20\n",
            "Model Number: 928 with model AverageValueNaive in generation 19 of 20\n",
            "Model Number: 929 with model AverageValueNaive in generation 19 of 20\n",
            "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 190') in model 929: AverageValueNaive\n",
            "Model Number: 930 with model SeasonalNaive in generation 19 of 20\n",
            "Model Number: 931 with model SeasonalNaive in generation 19 of 20\n",
            "Model Number: 932 with model SeasonalNaive in generation 19 of 20\n",
            "Model Number: 933 with model SeasonalNaive in generation 19 of 20\n",
            "Model Number: 934 with model WindowRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 935 with model WindowRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 936 with model WindowRegression in generation 19 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 936: WindowRegression\n",
            "Model Number: 937 with model WindowRegression in generation 19 of 20\n",
            "Model Number: 938 with model GLS in generation 19 of 20\n",
            "Model Number: 939 with model GLS in generation 19 of 20\n",
            "Model Number: 940 with model UnobservedComponents in generation 19 of 20\n",
            "Model Number: 941 with model UnobservedComponents in generation 19 of 20\n",
            "Model Number: 942 with model UnobservedComponents in generation 19 of 20\n",
            "Model Number: 943 with model GLM in generation 19 of 20\n",
            "Model Number: 944 with model GLM in generation 19 of 20\n",
            "Model Number: 945 with model GLM in generation 19 of 20\n",
            "Model Number: 946 with model GLM in generation 19 of 20\n",
            "New Generation: 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 947 with model FBProphet in generation 20 of 20\n",
            "Initial log joint probability = -24.871\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       132.147   5.38184e-05       80.1535           1           1      125   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     138        132.15   1.57843e-06       73.2383   2.009e-08       0.001      208  LS failed, Hessian reset \n",
            "     161        132.15   1.91538e-09       68.4179      0.1042      0.1042      240   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 948 with model FBProphet in generation 20 of 20\n",
            "Initial log joint probability = -7.60644\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      64        232.68   0.000201927       53.9212   2.405e-06       0.001      129  LS failed, Hessian reset \n",
            "      88       232.688   8.00857e-07       49.9829   1.147e-08       0.001      199  LS failed, Hessian reset \n",
            "      99       232.688   5.85506e-07       54.0871      0.7891      0.7891      215   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     123       232.696   5.36382e-05       52.0691   9.482e-07       0.001      289  LS failed, Hessian reset \n",
            "     158       232.698   1.59548e-05       42.9129   1.886e-07       0.001      383  LS failed, Hessian reset \n",
            "     170       232.699   7.77331e-09        63.982     0.04026     0.04026      400   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 949 with model FBProphet in generation 20 of 20\n",
            "Initial log joint probability = -9.94635\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       259.737   0.000328853       80.3819      0.8267      0.8267      123   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     113        260.13    0.00326986       60.0356   8.264e-05       0.001      175  LS failed, Hessian reset \n",
            "     190       260.672   7.59372e-05       82.0087   1.071e-06       0.001      310  LS failed, Hessian reset \n",
            "     199       260.673   6.47444e-07       69.7432     0.02758           1      325   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     228       260.674   5.69478e-06        66.756   6.425e-08       0.001      405  LS failed, Hessian reset \n",
            "     261       260.675   8.03437e-06       75.5435   1.097e-07       0.001      493  LS failed, Hessian reset \n",
            "     299       260.676    3.3827e-06       72.8066      0.2005           1      545   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     327       260.676   1.51593e-06       93.0864   2.055e-08       0.001      620  LS failed, Hessian reset \n",
            "     350       260.676   2.64555e-09       65.0005      0.1208      0.1208      655   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 950 with model DatepartRegression in generation 20 of 20\n",
            "Model Number: 951 with model DatepartRegression in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 952 with model DatepartRegression in generation 20 of 20\n",
            "Model Number: 953 with model RollingRegression in generation 20 of 20\n",
            "Template Eval Error: ValueError('array must not contain infs or NaNs') in model 953: RollingRegression\n",
            "Model Number: 954 with model RollingRegression in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_fastica.py:481: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_fastica.py:481: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 955 with model RollingRegression in generation 20 of 20\n",
            "Model Number: 956 with model RollingRegression in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2982: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in log\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 957 with model ETS in generation 20 of 20\n",
            "Model Number: 958 with model ETS in generation 20 of 20\n",
            "Model Number: 959 with model ETS in generation 20 of 20\n",
            "Model Number: 960 with model ETS in generation 20 of 20\n",
            "Model Number: 961 with model LastValueNaive in generation 20 of 20\n",
            "Model Number: 962 with model LastValueNaive in generation 20 of 20\n",
            "Model Number: 963 with model LastValueNaive in generation 20 of 20\n",
            "Model Number: 964 with model AverageValueNaive in generation 20 of 20\n",
            "Model Number: 965 with model AverageValueNaive in generation 20 of 20\n",
            "Model Number: 966 with model SeasonalNaive in generation 20 of 20\n",
            "Model Number: 967 with model SeasonalNaive in generation 20 of 20\n",
            "Model Number: 968 with model SeasonalNaive in generation 20 of 20\n",
            "Model Number: 969 with model SeasonalNaive in generation 20 of 20\n",
            "Model Number: 970 with model WindowRegression in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 971 with model WindowRegression in generation 20 of 20\n",
            "Model Number: 972 with model WindowRegression in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 973 with model WindowRegression in generation 20 of 20\n",
            "Model Number: 974 with model GLS in generation 20 of 20\n",
            "Model Number: 975 with model GLS in generation 20 of 20\n",
            "Model Number: 976 with model GLS in generation 20 of 20\n",
            "Model Number: 977 with model UnobservedComponents in generation 20 of 20\n",
            "Model Number: 978 with model UnobservedComponents in generation 20 of 20\n",
            "Model Number: 979 with model UnobservedComponents in generation 20 of 20\n",
            "Model Number: 980 with model GLM in generation 20 of 20\n",
            "Model Number: 981 with model GLM in generation 20 of 20\n",
            "Model Number: 982 with model GLM in generation 20 of 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 983 with model GLM in generation 20 of 20\n",
            "Model Number: 984 with model Ensemble in generation 0 of 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 985 with model Ensemble in generation 0 of 0\n",
            "Initial log joint probability = -7.27654\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       217.759     0.0114219       73.8054           1           1      133   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     121       218.555   0.000415244       88.7285    4.94e-06       0.001      198  LS failed, Hessian reset \n",
            "     199       218.804   1.13165e-05       77.6877      0.3688      0.3688      303   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     257       218.807   4.79898e-06       76.9309   5.503e-08       0.001      408  LS failed, Hessian reset \n",
            "     283       218.807   1.48458e-08       70.3377      0.0772           1      442   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -8.04323\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      87        208.26    0.00365605       97.7423   3.839e-05       0.001      151  LS failed, Hessian reset \n",
            "      99       208.484   5.80279e-05       76.5849      0.2187      0.2187      169   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     155       208.504   5.52018e-06       63.1576   6.696e-08       0.001      276  LS failed, Hessian reset \n",
            "     195       208.504   7.81882e-07       59.2164    9.74e-09       0.001      360  LS failed, Hessian reset \n",
            "     199       208.504   2.12495e-07        65.543       2.848      0.2848      366   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     209       208.504    3.0923e-09         84.78      0.1417      0.1417      380   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 986 with model Ensemble in generation 0 of 0\n",
            "Initial log joint probability = -8.74009\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      54       219.646   0.000768441       57.2851    1.14e-05       0.001      110  LS failed, Hessian reset \n",
            "      99       219.965    0.00214118       63.4694           1           1      171   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     107        220.15   0.000131754       39.1696   2.866e-06       0.001      255  LS failed, Hessian reset \n",
            "     199       220.343   0.000328278       65.9987           1           1      391   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     258       220.415   6.96218e-07       56.8868   1.135e-08       0.001      512  LS failed, Hessian reset \n",
            "     272       220.415   3.23573e-09       56.7538     0.06761           1      535   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Round: 1\n",
            "Model Number: 1 of 135 with model Ensemble for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 2 of 135 with model Ensemble for Validation 1\n",
            "Initial log joint probability = -7.35167\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       207.995   7.47391e-05        79.216      0.2987      0.2987      124   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     134       208.025   3.54314e-05       89.2149   3.752e-07       0.001      204  LS failed, Hessian reset \n",
            "     183       208.032   9.75163e-09       82.4639      0.5029      0.5029      268   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -7.50499\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       211.777    0.00387688       74.7465           1           1      131   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       212.225   9.83413e-07       64.6141           1           1      254   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       212.225   1.46292e-06       86.5457   1.617e-08       0.001      332  LS failed, Hessian reset \n",
            "     246       212.225   2.19218e-09       66.3642      0.2085      0.2085      353   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 3 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 4 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 5 of 135 with model Ensemble for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 6 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 7 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 8 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -9.80996\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       212.347    0.00379382        48.476      0.6617      0.6617      139   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     149       212.785   0.000137806        86.824   2.017e-06       0.001      250  LS failed, Hessian reset \n",
            "     199       212.812   0.000203807       49.5437      0.8416      0.8416      320   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     252       212.856   1.41705e-05       50.4649   2.914e-07       0.001      425  LS failed, Hessian reset \n",
            "     279       212.858   1.08159e-05       58.9068   1.512e-07       0.001      503  LS failed, Hessian reset \n",
            "     299       212.858   1.17749e-07       43.2983        2.87       0.287      541   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     321       212.858   1.67478e-07       55.5421   2.649e-09       0.001      610  LS failed, Hessian reset \n",
            "     328       212.858   9.80325e-09       48.1649      0.3651      0.3651      620   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 9 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.1628\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       203.827   0.000150203       67.5866      0.6181      0.6181      125   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       203.871   1.56223e-06       94.7079           1           1      255   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     266       203.967   0.000122589       103.842   1.282e-06       0.001      380  LS failed, Hessian reset \n",
            "     299       203.978   1.82599e-07       78.5935           1           1      421   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     332       203.979    7.0406e-07       100.044   7.947e-09       0.001      501  LS failed, Hessian reset \n",
            "     344       203.979   4.62472e-09       86.6532      0.2327      0.2327      516   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 10 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.31378\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      92       208.021    0.00149881       89.7606   1.443e-05       0.001      151  LS failed, Hessian reset \n",
            "      99       208.079   0.000305098       63.2581       2.752      0.2752      161   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     155       208.133   4.31297e-05       99.2844   4.448e-07       0.001      270  LS failed, Hessian reset \n",
            "     199       208.135   1.33525e-07       89.5392      0.3021      0.3021      328   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     219       208.135   6.77521e-09       72.8809      0.7014      0.7014      353   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 11 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.35167\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       207.995   7.47391e-05        79.216      0.2987      0.2987      124   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     134       208.025   3.54314e-05       89.2149   3.752e-07       0.001      204  LS failed, Hessian reset \n",
            "     183       208.032   9.75163e-09       82.4639      0.5029      0.5029      268   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 12 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.35167\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      77       204.285   9.86583e-05       83.4453   1.108e-06       0.001      147  LS failed, Hessian reset \n",
            "      99       204.289   1.19833e-06       80.6643           1           1      178   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       204.675   2.08644e-05       76.3104      0.5007      0.5007      303   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     272        204.72   1.34662e-05       48.1564   1.506e-07       0.001      436  LS failed, Hessian reset \n",
            "     299       204.721   3.22555e-08       74.1325      0.6147      0.6147      472   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     304       204.721    5.3523e-09       60.5752      0.4754      0.4754      477   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 13 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.35167\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      77       204.285   9.86583e-05       83.4453   1.108e-06       0.001      147  LS failed, Hessian reset \n",
            "      99       204.289   1.19833e-06       80.6643           1           1      178   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       204.675   2.08644e-05       76.3104      0.5007      0.5007      303   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     272        204.72   1.34662e-05       48.1564   1.506e-07       0.001      436  LS failed, Hessian reset \n",
            "     299       204.721   3.22555e-08       74.1325      0.6147      0.6147      472   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     304       204.721    5.3523e-09       60.5752      0.4754      0.4754      477   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 14 of 135 with model FBProphet for Validation 1\n",
            "Initial log joint probability = -7.50499\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       211.777    0.00387688       74.7465           1           1      131   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       212.225   9.83413e-07       64.6141           1           1      254   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     231       212.225   1.46292e-06       86.5457   1.617e-08       0.001      332  LS failed, Hessian reset \n",
            "     246       212.225   2.19218e-09       66.3642      0.2085      0.2085      353   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 15 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 16 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 17 of 135 with model DatepartRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 18 of 135 with model DatepartRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 19 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 20 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 21 of 135 with model DatepartRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 22 of 135 with model DatepartRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 23 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 24 of 135 with model DatepartRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 25 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 26 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 27 of 135 with model ETS for Validation 1\n",
            "Model Number: 28 of 135 with model ETS for Validation 1\n",
            "Model Number: 29 of 135 with model ETS for Validation 1\n",
            "Model Number: 30 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 31 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 32 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 33 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 34 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 35 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 36 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 37 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 38 of 135 with model RollingRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 39 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 40 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 41 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 42 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 43 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 44 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 45 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 46 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 47 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 48 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 49 of 135 with model ETS for Validation 1\n",
            "Model Number: 50 of 135 with model GLS for Validation 1\n",
            "Model Number: 51 of 135 with model GLS for Validation 1\n",
            "Model Number: 52 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 53 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 54 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 55 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 56 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 57 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 58 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 59 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 60 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 61 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 62 of 135 with model UnobservedComponents for Validation 1\n",
            "Model Number: 63 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 64 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 65 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 66 of 135 with model ETS for Validation 1\n",
            "Model Number: 67 of 135 with model GLS for Validation 1\n",
            "Model Number: 68 of 135 with model GLS for Validation 1\n",
            "Model Number: 69 of 135 with model GLS for Validation 1\n",
            "Model Number: 70 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 71 of 135 with model GLM for Validation 1\n",
            "Model Number: 72 of 135 with model GLM for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 73 of 135 with model GLM for Validation 1\n",
            "Model Number: 74 of 135 with model GLM for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 75 of 135 with model GLM for Validation 1\n",
            "Model Number: 76 of 135 with model GLM for Validation 1\n",
            "Model Number: 77 of 135 with model GLM for Validation 1"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Number: 78 of 135 with model ETS for Validation 1\n",
            "Model Number: 79 of 135 with model GLM for Validation 1\n",
            "Model Number: 80 of 135 with model GLM for Validation 1\n",
            "Model Number: 81 of 135 with model GLM for Validation 1\n",
            "Model Number: 82 of 135 with model ETS for Validation 1\n",
            "Model Number: 83 of 135 with model GLS for Validation 1\n",
            "Model Number: 84 of 135 with model GLM for Validation 1\n",
            "Model Number: 85 of 135 with model GLS for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 86 of 135 with model GLS for Validation 1\n",
            "Model Number: 87 of 135 with model GLS for Validation 1\n",
            "Model Number: 88 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 89 of 135 with model ETS for Validation 1\n",
            "Model Number: 90 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 91 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 92 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 93 of 135 with model ETS for Validation 1\n",
            "Model Number: 94 of 135 with model SeasonalNaive for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/minpack.py:808: OptimizeWarning:\n",
            "\n",
            "Covariance of the parameters could not be estimated\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 95 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 96 of 135 with model GLS for Validation 1\n",
            "Model Number: 97 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 98 of 135 with model ETS for Validation 1\n",
            "Model Number: 99 of 135 with model ETS for Validation 1\n",
            "Model Number: 100 of 135 with model GLS for Validation 1\n",
            "Model Number: 101 of 135 with model SeasonalNaive for Validation 1\n",
            "Model Number: 102 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 103 of 135 with model WindowRegression for Validation 1\n",
            "FillNA method not known, returning original\n",
            "Model Number: 104 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 105 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 106 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 107 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 108 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 109 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 110 of 135 with model LastValueNaive for Validation 1\n",
            "Model Number: 111 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 112 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 113 of 135 with model RollingRegression for Validation 1\n",
            "Model Number: 114 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 115 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 116 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 117 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 118 of 135 with model WindowRegression for Validation 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 119 of 135 with model WindowRegression for Validation 1\n",
            "Model Number: 120 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 121 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 122 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 123 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 124 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 125 of 135 with model DatepartRegression for Validation 1\n",
            "Model Number: 126 of 135 with model AverageValueNaive for Validation 1\n",
            "Model Number: 127 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 128 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 129 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 130 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 131 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 132 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 133 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 134 of 135 with model ZeroesNaive for Validation 1\n",
            "Model Number: 135 of 135 with model ZeroesNaive for Validation 1\n",
            "Validation Round: 2\n",
            "Model Number: 1 of 135 with model Ensemble for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 2 of 135 with model Ensemble for Validation 2\n",
            "Initial log joint probability = -7.58357\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       195.721   0.000136183       86.3176           1           1      126   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     147        195.73   6.08151e-09       86.0504      0.5034      0.5034      187   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -7.77512\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       199.162    0.00333825       89.5372           1           1      129   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     145       199.432    0.00081983       90.2453   8.696e-06       0.001      217  LS failed, Hessian reset \n",
            "     185       199.503   4.46676e-06       91.7377   4.911e-08       0.001      305  LS failed, Hessian reset \n",
            "     199       199.504   4.80303e-08       80.2485      0.7553      0.7553      322   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     204       199.504   6.60662e-09       75.0682      0.2641      0.2641      330   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 3 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 4 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 5 of 135 with model Ensemble for Validation 2\n",
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 6 of 135 with model FBProphet for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n",
            "Model Number: 7 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 8 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -9.33386\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       200.736   0.000846522       87.9733      0.6822      0.6822      137   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     199       202.061   0.000868643       78.0842           1           1      260   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     202       202.062   3.10019e-05       54.4559    4.79e-07       0.001      309  LS failed, Hessian reset \n",
            "     234       202.063   2.34292e-07       58.2241    3.17e-09       0.001      404  LS failed, Hessian reset \n",
            "     239       202.063   1.20509e-08       43.5687      0.5924      0.5924      409   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: relative gradient magnitude is below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 9 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.31682\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       191.266   2.84721e-07       84.0428      0.5164      0.5164      130   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     152       191.315   0.000212073       103.279   2.073e-06       0.001      256  LS failed, Hessian reset \n",
            "     199       191.353   3.73939e-06       78.0099      0.3621           1      314   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     214       191.359   6.95422e-05       91.2597   7.493e-07       0.001      379  LS failed, Hessian reset \n",
            "     299       191.371   2.92141e-05       99.8273           1           1      487   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     300       191.371   6.28262e-06       91.2223   6.293e-08       0.001      527  LS failed, Hessian reset \n",
            "     326       191.372   9.06219e-09       79.9871      0.2938      0.2938      558   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 10 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.5941\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       194.782   0.000869078       94.7622      0.1258      0.7171      132   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     157       194.878   9.13695e-05       115.813   9.311e-07       0.001      271  LS failed, Hessian reset \n",
            "     199       194.887   4.45213e-08       93.1079      0.6715      0.6715      329   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     226       194.887   9.69324e-09       86.1479      0.2729      0.2729      361   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 11 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.58357\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       195.721   0.000136183       86.3176           1           1      126   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     147        195.73   6.08151e-09       86.0504      0.5034      0.5034      187   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 12 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.58357\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99        192.71   7.61251e-07       70.8038      0.4356           1      127   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     118       192.726   0.000154876       81.3281   1.636e-06       0.001      202  LS failed, Hessian reset \n",
            "     168       192.738   9.09186e-09       73.8778      0.3551           1      272   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 13 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.58357\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99        192.71   7.61251e-07       70.8038      0.4356           1      127   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     118       192.726   0.000154876       81.3281   1.636e-06       0.001      202  LS failed, Hessian reset \n",
            "     168       192.738   9.09186e-09       73.8778      0.3551           1      272   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 14 of 135 with model FBProphet for Validation 2\n",
            "Initial log joint probability = -7.77512\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      99       199.162    0.00333825       89.5372           1           1      129   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     145       199.432    0.00081983       90.2453   8.696e-06       0.001      217  LS failed, Hessian reset \n",
            "     185       199.503   4.46676e-06       91.7377   4.911e-08       0.001      305  LS failed, Hessian reset \n",
            "     199       199.504   4.80303e-08       80.2485      0.7553      0.7553      322   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     204       199.504   6.60662e-09       75.0682      0.2641      0.2641      330   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "Model Number: 15 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 16 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 17 of 135 with model DatepartRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 18 of 135 with model DatepartRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 19 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 20 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 21 of 135 with model DatepartRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 22 of 135 with model DatepartRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 23 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 24 of 135 with model DatepartRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:1505: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 25 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 26 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 27 of 135 with model ETS for Validation 2\n",
            "Model Number: 28 of 135 with model ETS for Validation 2\n",
            "Model Number: 29 of 135 with model ETS for Validation 2\n",
            "Model Number: 30 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 31 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 32 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 33 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 34 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 35 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 36 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 37 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 38 of 135 with model RollingRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/autots/models/sklearn.py:680: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 39 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 40 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 41 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 42 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 43 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 44 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 45 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 46 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 47 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 48 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 49 of 135 with model ETS for Validation 2\n",
            "Model Number: 50 of 135 with model GLS for Validation 2\n",
            "Model Number: 51 of 135 with model GLS for Validation 2\n",
            "Model Number: 52 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 53 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 54 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 55 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 56 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 57 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 58 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 59 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 60 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 61 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 62 of 135 with model UnobservedComponents for Validation 2\n",
            "Model Number: 63 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 64 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 65 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 66 of 135 with model ETS for Validation 2\n",
            "Model Number: 67 of 135 with model GLS for Validation 2\n",
            "Model Number: 68 of 135 with model GLS for Validation 2\n",
            "Model Number: 69 of 135 with model GLS for Validation 2\n",
            "Model Number: 70 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 71 of 135 with model GLM for Validation 2\n",
            "Model Number: 72 of 135 with model GLM for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 73 of 135 with model GLM for Validation 2\n",
            "Model Number: 74 of 135 with model GLM for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 75 of 135 with model GLM for Validation 2\n",
            "Model Number: 76 of 135 with model GLM for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 77 of 135 with model GLM for Validation 2\n",
            "Model Number: 78 of 135 with model ETS for Validation 2\n",
            "Model Number: 79 of 135 with model GLM for Validation 2\n",
            "Model Number: 80 of 135 with model GLM for Validation 2\n",
            "Model Number: 81 of 135 with model GLM for Validation 2\n",
            "Model Number: 82 of 135 with model ETS for Validation 2\n",
            "Model Number: 83 of 135 with model GLS for Validation 2\n",
            "Model Number: 84 of 135 with model GLM for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/links.py:190: RuntimeWarning:\n",
            "\n",
            "overflow encountered in exp\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/statsmodels/genmod/families/family.py:889: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in true_divide\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 85 of 135 with model GLS for Validation 2\n",
            "Model Number: 86 of 135 with model GLS for Validation 2\n",
            "Model Number: 87 of 135 with model GLS for Validation 2\n",
            "Model Number: 88 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 89 of 135 with model ETS for Validation 2\n",
            "Model Number: 90 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 91 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 92 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 93 of 135 with model ETS for Validation 2\n",
            "Model Number: 94 of 135 with model SeasonalNaive for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/minpack.py:808: OptimizeWarning:\n",
            "\n",
            "Covariance of the parameters could not be estimated\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 95 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 96 of 135 with model GLS for Validation 2\n",
            "Model Number: 97 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 98 of 135 with model ETS for Validation 2\n",
            "Model Number: 99 of 135 with model ETS for Validation 2\n",
            "Model Number: 100 of 135 with model GLS for Validation 2\n",
            "Model Number: 101 of 135 with model SeasonalNaive for Validation 2\n",
            "Model Number: 102 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 103 of 135 with model WindowRegression for Validation 2\n",
            "FillNA method not known, returning original\n",
            "Model Number: 104 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 105 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 106 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 107 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 108 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 109 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 110 of 135 with model LastValueNaive for Validation 2\n",
            "Model Number: 111 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 112 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 113 of 135 with model RollingRegression for Validation 2\n",
            "Model Number: 114 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 115 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 116 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 117 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 118 of 135 with model WindowRegression for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning:\n",
            "\n",
            "lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Number: 119 of 135 with model WindowRegression for Validation 2\n",
            "Model Number: 120 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 121 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 122 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 123 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 124 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 125 of 135 with model DatepartRegression for Validation 2\n",
            "Model Number: 126 of 135 with model AverageValueNaive for Validation 2\n",
            "Model Number: 127 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 128 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 129 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 130 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 131 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 132 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 133 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 134 of 135 with model ZeroesNaive for Validation 2\n",
            "Model Number: 135 of 135 with model ZeroesNaive for Validation 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initial log joint probability = -9.77464\n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "      50       223.702    0.00165871       63.8065    3.04e-05       0.001       98  LS failed, Hessian reset \n",
            "      99       224.275   0.000710305       81.0855     0.08025           1      162   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     138       224.403   8.22733e-06       58.2784   1.196e-07       0.001      261  LS failed, Hessian reset \n",
            "     177       224.403   1.77955e-06       51.2741   2.344e-08       0.001      350  LS failed, Hessian reset \n",
            "     199       224.403   7.53726e-07       70.6202           1           1      377   \n",
            "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
            "     253       224.414   1.74115e-06       69.5188   3.521e-08       0.001      510  LS failed, Hessian reset \n",
            "     267       224.414   9.34257e-09       55.6722      0.5222      0.5222      536   \n",
            "Optimization terminated normally: \n",
            "  Convergence detected: absolute parameter change was below tolerance\n",
            "DogeCoin Price Prediction\n",
            "            Close\n",
            "2021-06-04    NaN\n",
            "2021-06-05    NaN\n",
            "2021-06-06    NaN\n",
            "2021-06-07    NaN\n",
            "2021-06-08    NaN\n",
            "2021-06-09    NaN\n",
            "2021-06-10    NaN\n",
            "2021-06-11    NaN\n",
            "2021-06-12    NaN\n",
            "2021-06-13    NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Ipo0eySaUC"
      },
      "source": [
        "**MODELO DE APRENDIZAJE AUTOMÁTICO DE EXTREMO A EXTREMO CON PYTHON**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYwGfsT9XvNg"
      },
      "source": [
        "Para poder comenzar utilizamos la biblioteca streamlit en Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rs_rGTTSQDK",
        "outputId": "ba3e32ac-e5ad-4de6-a6b9-13ec77423565"
      },
      "source": [
        "pip install streamlit"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.84.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.18)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.6.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.0.1)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit) (4.0.0)\n",
            "Requirement already satisfied: debugpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: importlib-metadata<4; python_version < \"3.8.0\" in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (3.10.1)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (7.25.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4; python_version < \"3.8.0\"->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (3.4.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (3.0.19)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (57.0.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.7.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkehGGm5rA1k"
      },
      "source": [
        "!pip install streamlit -q"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ql8GR_S3r9"
      },
      "source": [
        "Implementamos un modelo de aplicación de un extremo a otro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBLp3aXfX9sR",
        "outputId": "a4c6c8f3-5160-44df-edc5-3ab70eaf6c25"
      },
      "source": [
        "from autots import AutoTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from seaborn import regression\n",
        "sns.set()\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "import streamlit as st\n",
        "st.title(\"Future Price Prediction Model\")\n",
        "df = st.text_input(\"Let's Predict the Future Prices\")\n",
        "\n",
        "if df == \"Dogecoin\":\n",
        "    data = pd.read_csv(\"Dogecoin.csv\")\n",
        "    print(data.head())\n",
        "    data.dropna()\n",
        "    model = AutoTS(forecast_length=10, frequency='infer', ensemble='simple', drop_data_older_than_periods=200)\n",
        "    model = model.fit(data, date_col='Date', value_col='Close', id_col=None)\n",
        "    prediction = model.predict()\n",
        "    forecast = prediction.forecast\n",
        "    st.write(forecast)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:\n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2021-07-14 19:59:00.902 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXJmw9QXu4Jz",
        "outputId": "d1a7bf1a-e887-4cb9-e5ee-d266044f723b"
      },
      "source": [
        "!streamlit run Proyecto.py "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "\n",
            "Error: Invalid value: File does not exist: Proyecto.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}